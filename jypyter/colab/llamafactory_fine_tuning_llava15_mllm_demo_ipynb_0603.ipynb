{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t30RA5uglk71"
      },
      "source": [
        "# LLaMA-Factory LoRA Fine-Tuning llava1_5\n",
        "https://github.com/hiyouga/LLaMA-Factory/tree/main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H0Hjeailk75"
      },
      "source": [
        "## init LLaMA Factory\n",
        "pip部署LLaMA Factory环境(不需要)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVSTm0gNlk76"
      },
      "source": [
        "## login huggingface\n",
        "不需要GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dXiZlGLQlk76"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import HfApi, HfFolder\n",
        "\n",
        "# # 替换为你的 Hugging Face 用户令牌\n",
        "# token = \"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\"\n",
        "\n",
        "# # 保存令牌\n",
        "# HfFolder.save_token(token)\n",
        "\n",
        "# # 创建 HfApi 实例\n",
        "# api = HfApi()\n",
        "\n",
        "# # 验证登录是否成功\n",
        "# user_info = api.whoami()\n",
        "# print(\"Successfully logged in as:\", user_info[\"name\"])\n",
        "\n",
        "HF_TOKEN = \"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 部署LLaMA-Factory环境\n",
        "不需要GPU"
      ],
      "metadata": {
        "id": "6ePTx-QlC_sx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DffFSWgclk7-",
        "outputId": "81fbd9d9-eebf-49d0-ee70-974e17f3e538",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 259, done.\u001b[K\n",
            "remote: Counting objects: 100% (259/259), done.\u001b[K\n",
            "remote: Compressing objects: 100% (220/220), done.\u001b[K\n",
            "remote: Total 259 (delta 45), reused 141 (delta 28), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (259/259), 7.77 MiB | 13.05 MiB/s, done.\n",
            "Resolving deltas: 100% (45/45), done.\n",
            "/content/LLaMA-Factory\n",
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers>=4.37.2 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (4.41.1)\n",
            "Collecting datasets>=2.14.3 (from llamafactory==0.7.2.dev0)\n",
            "  Downloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate>=0.27.2 (from llamafactory==0.7.2.dev0)\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft>=0.10.0 (from llamafactory==0.7.2.dev0)\n",
            "  Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl>=0.8.1 (from llamafactory==0.7.2.dev0)\n",
            "  Downloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio>=4.0.0 (from llamafactory==0.7.2.dev0)\n",
            "  Downloading gradio-4.32.2-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (1.11.4)\n",
            "Collecting einops (from llamafactory==0.7.2.dev0)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (3.20.3)\n",
            "Collecting uvicorn (from llamafactory==0.7.2.dev0)\n",
            "  Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (2.7.1)\n",
            "Collecting fastapi (from llamafactory==0.7.2.dev0)\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sse-starlette (from llamafactory==0.7.2.dev0)\n",
            "  Downloading sse_starlette-2.1.0-py3-none-any.whl (9.2 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (3.7.1)\n",
            "Collecting fire (from llamafactory==0.7.2.dev0)\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (24.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (6.0.1)\n",
            "Collecting bitsandbytes>=0.39.0 (from llamafactory==0.7.2.dev0)\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (3.8.1)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (0.42.1)\n",
            "Collecting rouge-chinese (from llamafactory==0.7.2.dev0)\n",
            "  Downloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.2->llamafactory==0.7.2.dev0) (1.25.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.2->llamafactory==0.7.2.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.2->llamafactory==0.7.2.dev0) (0.23.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.2->llamafactory==0.7.2.dev0) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.14.3->llamafactory==0.7.2.dev0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (2.0.3)\n",
            "Collecting requests>=2.32.1 (from datasets>=2.14.3->llamafactory==0.7.2.dev0)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (4.66.4)\n",
            "Collecting xxhash (from datasets>=2.14.3->llamafactory==0.7.2.dev0)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=2.14.3->llamafactory==0.7.2.dev0)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (3.9.5)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (4.2.2)\n",
            "Collecting ffmpy (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.17.0 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading gradio_client-0.17.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.3/316.3 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.24.1 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (2.1.5)\n",
            "Collecting orjson~=3.0 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (9.4.0)\n",
            "Collecting pydub (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading ruff-0.4.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Collecting typer<1.0,>=0.12 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (4.11.0)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (2.0.7)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.17.0->gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (2.8.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.7.2.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.7.2.dev0) (2.18.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.7.2.dev0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.7.2.dev0) (3.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.7.2.dev0) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->llamafactory==0.7.2.dev0) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->llamafactory==0.7.2.dev0) (0.19.1)\n",
            "Collecting tyro>=0.5.11 (from trl>=0.8.1->llamafactory==0.7.2.dev0)\n",
            "  Downloading tyro-0.8.4-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.7.2.dev0) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn->llamafactory==0.7.2.dev0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.38.0,>=0.37.2 (from fastapi->llamafactory==0.7.2.dev0)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi->llamafactory==0.7.2.dev0)\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->llamafactory==0.7.2.dev0)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi->llamafactory==0.7.2.dev0)\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.7.2.dev0) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.7.2.dev0) (2.4.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->llamafactory==0.7.2.dev0) (1.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from sse-starlette->llamafactory==0.7.2.dev0) (3.7.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.12.1)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->llamafactory==0.7.2.dev0)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi->llamafactory==0.7.2.dev0) (3.7)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (4.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.7.2.dev0) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.7.2.dev0) (1.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.14.3->llamafactory==0.7.2.dev0) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.14.3->llamafactory==0.7.2.dev0) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets>=2.14.3->llamafactory==0.7.2.dev0) (3.3.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->sse-starlette->llamafactory==0.7.2.dev0) (1.2.1)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.7.2.dev0) (13.7.1)\n",
            "Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl>=0.8.1->llamafactory==0.7.2.dev0) (0.16)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.1->llamafactory==0.7.2.dev0)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Collecting httptools>=0.5.0 (from uvicorn->llamafactory==0.7.2.dev0)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn->llamafactory==0.7.2.dev0)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn->llamafactory==0.7.2.dev0)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn->llamafactory==0.7.2.dev0)\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->llamafactory==0.7.2.dev0) (1.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.18.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.7.2.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.7.2.dev0) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.1.2)\n",
            "Building wheels for collected packages: fire, llamafactory, ffmpy\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117029 sha256=de018e9f3cf2a51d001579a5366b1d31bec5bd23436f2323169c021202b4d7b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.7.2.dev0-0.editable-py3-none-any.whl size=18708 sha256=40b5008851ff6052b3efc01c9440757915f9d27047f7badb2bd82f9cd977653b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dz5znog7/wheels/de/aa/c5/27b5682c5592b7c0eecc3e208f176dedf6b11a61cf2a910b85\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=c1799e44463ca40ed3d078ecf4e340be9164e44109683e0681e68936a7d131a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built fire llamafactory ffmpy\n",
            "Installing collected packages: pydub, ffmpy, xxhash, websockets, uvloop, ujson, tomlkit, shtab, shellingham, semantic-version, ruff, rouge-chinese, requests, python-multipart, python-dotenv, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, httptools, h11, fire, einops, dnspython, dill, aiofiles, watchfiles, uvicorn, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, httpcore, email_validator, tyro, typer, sse-starlette, nvidia-cusolver-cu12, httpx, gradio-client, fastapi-cli, datasets, fastapi, bitsandbytes, accelerate, trl, peft, gradio, llamafactory\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.4\n",
            "    Uninstalling typer-0.9.4:\n",
            "      Successfully uninstalled typer-0.9.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.30.1 aiofiles-23.2.1 bitsandbytes-0.43.1 datasets-2.19.2 dill-0.3.8 dnspython-2.6.1 einops-0.8.0 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.4 ffmpy-0.3.2 fire-0.6.0 gradio-4.32.2 gradio-client-0.17.0 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 llamafactory-0.7.2.dev0 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 orjson-3.10.3 peft-0.11.1 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.9 requests-2.32.3 rouge-chinese-1.0.3 ruff-0.4.7 semantic-version-2.10.0 shellingham-1.5.4 shtab-1.7.1 sse-starlette-2.1.0 starlette-0.37.2 tomlkit-0.12.0 trl-0.8.6 typer-0.12.3 tyro-0.8.4 ujson-5.10.0 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-11.0.3 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%pip install -e .[torch,metrics,bitsandbytes]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-ZBoMa4lk7-"
      },
      "source": [
        "## 确认GPU环境"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xbLZSCZklk7_",
        "outputId": "5ea91063-d191-4856-da69-a05607f18211",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 环境满足\n",
            "Mon Jun  3 08:42:22 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P0              33W /  70W |  15019MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "  print(\"GPU 环境满足\")\n",
        "  !nvidia-smi\n",
        "except AssertionError:\n",
        "  print(\"需要 GPU 环境\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 开始微调 llava1_5_lora_sft.yaml"
      ],
      "metadata": {
        "id": "k83CKs9vDr6i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IKO7RsMLlk8A",
        "outputId": "edcad42f-c9bf-457e-96e7-be7c835d29fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-06-03 07:50:54.296172: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-03 07:50:54.296218: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-03 07:50:54.444516: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-06-03 07:50:54.569480: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-03 07:50:55.639204: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "06/03/2024 07:51:02 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "tokenizer_config.json: 100% 1.33k/1.33k [00:00<00:00, 8.90MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 6.00MB/s]\n",
            "tokenizer.json: 100% 1.84M/1.84M [00:00<00:00, 12.2MB/s]\n",
            "added_tokens.json: 100% 41.0/41.0 [00:00<00:00, 388kB/s]\n",
            "special_tokens_map.json: 100% 438/438 [00:00<00:00, 3.24MB/s]\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-03 07:51:03,180 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-03 07:51:03,180 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-03 07:51:03,180 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-03 07:51:03,181 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-03 07:51:03,181 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-06-03 07:51:03,262 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "preprocessor_config.json: 100% 557/557 [00:00<00:00, 4.72MB/s]\n",
            "[INFO|image_processing_utils.py:374] 2024-06-03 07:51:03,501 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/preprocessor_config.json\n",
            "[INFO|image_processing_utils.py:374] 2024-06-03 07:51:03,590 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/preprocessor_config.json\n",
            "[INFO|image_processing_utils.py:424] 2024-06-03 07:51:03,592 >> Image processor CLIPImageProcessor {\n",
            "  \"_valid_processor_keys\": [\n",
            "    \"images\",\n",
            "    \"do_resize\",\n",
            "    \"size\",\n",
            "    \"resample\",\n",
            "    \"do_center_crop\",\n",
            "    \"crop_size\",\n",
            "    \"do_rescale\",\n",
            "    \"rescale_factor\",\n",
            "    \"do_normalize\",\n",
            "    \"image_mean\",\n",
            "    \"image_std\",\n",
            "    \"do_convert_rgb\",\n",
            "    \"return_tensors\",\n",
            "    \"data_format\",\n",
            "    \"input_data_format\"\n",
            "  ],\n",
            "  \"crop_size\": {\n",
            "    \"height\": 336,\n",
            "    \"width\": 336\n",
            "  },\n",
            "  \"do_center_crop\": true,\n",
            "  \"do_convert_rgb\": true,\n",
            "  \"do_normalize\": true,\n",
            "  \"do_rescale\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"image_mean\": [\n",
            "    0.48145466,\n",
            "    0.4578275,\n",
            "    0.40821073\n",
            "  ],\n",
            "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
            "  \"image_std\": [\n",
            "    0.26862954,\n",
            "    0.26130258,\n",
            "    0.27577711\n",
            "  ],\n",
            "  \"processor_class\": \"LlavaProcessor\",\n",
            "  \"resample\": 3,\n",
            "  \"rescale_factor\": 0.00392156862745098,\n",
            "  \"size\": {\n",
            "    \"shortest_edge\": 336\n",
            "  }\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-03 07:51:03,649 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-03 07:51:03,649 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-03 07:51:03,649 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-03 07:51:03,649 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-03 07:51:03,649 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-06-03 07:51:03,705 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|processing_utils.py:400] 2024-06-03 07:51:03,857 >> Processor LlavaProcessor:\n",
            "- image_processor: CLIPImageProcessor {\n",
            "  \"_valid_processor_keys\": [\n",
            "    \"images\",\n",
            "    \"do_resize\",\n",
            "    \"size\",\n",
            "    \"resample\",\n",
            "    \"do_center_crop\",\n",
            "    \"crop_size\",\n",
            "    \"do_rescale\",\n",
            "    \"rescale_factor\",\n",
            "    \"do_normalize\",\n",
            "    \"image_mean\",\n",
            "    \"image_std\",\n",
            "    \"do_convert_rgb\",\n",
            "    \"return_tensors\",\n",
            "    \"data_format\",\n",
            "    \"input_data_format\"\n",
            "  ],\n",
            "  \"crop_size\": {\n",
            "    \"height\": 336,\n",
            "    \"width\": 336\n",
            "  },\n",
            "  \"do_center_crop\": true,\n",
            "  \"do_convert_rgb\": true,\n",
            "  \"do_normalize\": true,\n",
            "  \"do_rescale\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"image_mean\": [\n",
            "    0.48145466,\n",
            "    0.4578275,\n",
            "    0.40821073\n",
            "  ],\n",
            "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
            "  \"image_std\": [\n",
            "    0.26862954,\n",
            "    0.26130258,\n",
            "    0.27577711\n",
            "  ],\n",
            "  \"processor_class\": \"LlavaProcessor\",\n",
            "  \"resample\": 3,\n",
            "  \"rescale_factor\": 0.00392156862745098,\n",
            "  \"size\": {\n",
            "    \"shortest_edge\": 336\n",
            "  }\n",
            "}\n",
            "\n",
            "- tokenizer: LlamaTokenizerFast(name_or_path='llava-hf/llava-1.5-7b-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32000: AddedToken(\"<image>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32001: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "\n",
            "{\n",
            "  \"processor_class\": \"LlavaProcessor\"\n",
            "}\n",
            "\n",
            "06/03/2024 07:51:03 - INFO - llamafactory.data.loader - Loading dataset mllm_demo.json...\n",
            "Generating train split: 6 examples [00:00, 305.59 examples/s]\n",
            "num_proc must be <= 6. Reducing num_proc to 6 for dataset of size 6.\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=6): 100% 6/6 [00:00<00:00, 21.80 examples/s]\n",
            "num_proc must be <= 6. Reducing num_proc to 6 for dataset of size 6.\n",
            "Running tokenizer on dataset (num_proc=6): 100% 6/6 [00:01<00:00,  4.11 examples/s]\n",
            "input_ids:\n",
            "[319, 13563, 1546, 263, 12758, 1404, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 1404, 29915, 29879, 5155, 29889, 3148, 1001, 29901, 29871, 32000, 11644, 526, 896, 29973, 319, 1799, 9047, 13566, 29901, 2688, 29915, 276, 476, 1662, 322, 402, 2267, 29920, 1335, 515, 19584, 13564, 436, 636, 2, 3148, 1001, 29901, 1724, 526, 896, 2599, 29973, 319, 1799, 9047, 13566, 29901, 2688, 526, 10894, 1218, 373, 278, 269, 11953, 1746, 29889, 2]\n",
            "inputs:\n",
            "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image> Who are they? ASSISTANT: They're Kane and Gretzka from Bayern Munich..</s> USER: What are they doing? ASSISTANT: They are celebrating on the soccer field.</s>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2688, 29915, 276, 476, 1662, 322, 402, 2267, 29920, 1335, 515, 19584, 13564, 436, 636, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2688, 526, 10894, 1218, 373, 278, 269, 11953, 1746, 29889, 2]\n",
            "labels:\n",
            "They're Kane and Gretzka from Bayern Munich..</s> They are celebrating on the soccer field.</s>\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "config.json: 100% 950/950 [00:00<00:00, 5.77MB/s]\n",
            "[INFO|configuration_utils.py:733] 2024-06-03 07:51:06,536 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/config.json\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:100: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:796] 2024-06-03 07:51:06,551 >> Model config LlavaConfig {\n",
            "  \"_name_or_path\": \"llava-hf/llava-1.5-7b-hf\",\n",
            "  \"architectures\": [\n",
            "    \"LlavaForConditionalGeneration\"\n",
            "  ],\n",
            "  \"ignore_index\": -100,\n",
            "  \"image_token_index\": 32000,\n",
            "  \"model_type\": \"llava\",\n",
            "  \"pad_token_id\": 32001,\n",
            "  \"projector_hidden_act\": \"gelu\",\n",
            "  \"text_config\": {\n",
            "    \"_name_or_path\": \"lmsys/vicuna-7b-v1.5\",\n",
            "    \"architectures\": [\n",
            "      \"LlamaForCausalLM\"\n",
            "    ],\n",
            "    \"max_position_embeddings\": 4096,\n",
            "    \"model_type\": \"llama\",\n",
            "    \"rms_norm_eps\": 1e-05,\n",
            "    \"torch_dtype\": \"float16\",\n",
            "    \"vocab_size\": 32064\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.41.1\",\n",
            "  \"vision_config\": {\n",
            "    \"hidden_size\": 1024,\n",
            "    \"image_size\": 336,\n",
            "    \"intermediate_size\": 4096,\n",
            "    \"model_type\": \"clip_vision_model\",\n",
            "    \"num_attention_heads\": 16,\n",
            "    \"num_hidden_layers\": 24,\n",
            "    \"patch_size\": 14,\n",
            "    \"projection_dim\": 768,\n",
            "    \"vocab_size\": 32000\n",
            "  },\n",
            "  \"vision_feature_layer\": -2,\n",
            "  \"vision_feature_select_strategy\": \"default\"\n",
            "}\n",
            "\n",
            "model.safetensors.index.json: 100% 70.1k/70.1k [00:00<00:00, 184MB/s]\n",
            "[INFO|modeling_utils.py:3474] 2024-06-03 07:51:06,827 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/model.safetensors.index.json\n",
            "Downloading shards:   0% 0/3 [00:00<?, ?it/s]\n",
            "model-00001-of-00003.safetensors:   0% 0.00/4.99G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   0% 21.0M/4.99G [00:00<00:38, 130MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   1% 41.9M/4.99G [00:00<00:32, 153MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   1% 62.9M/4.99G [00:00<00:28, 172MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   2% 83.9M/4.99G [00:00<00:26, 183MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   2% 105M/4.99G [00:00<00:25, 189MB/s] \u001b[A\n",
            "model-00001-of-00003.safetensors:   3% 126M/4.99G [00:00<00:26, 186MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   3% 147M/4.99G [00:00<00:26, 184MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   3% 168M/4.99G [00:00<00:25, 190MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   4% 189M/4.99G [00:01<00:25, 191MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   4% 210M/4.99G [00:01<00:24, 194MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   5% 231M/4.99G [00:01<00:24, 195MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   5% 252M/4.99G [00:01<00:24, 194MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   5% 273M/4.99G [00:01<00:25, 183MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   6% 294M/4.99G [00:01<00:26, 177MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   6% 315M/4.99G [00:01<00:26, 174MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   7% 336M/4.99G [00:01<00:26, 174MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   7% 357M/4.99G [00:01<00:26, 174MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   8% 377M/4.99G [00:02<00:26, 172MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   8% 398M/4.99G [00:02<00:27, 170MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   8% 419M/4.99G [00:02<00:26, 171MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   9% 440M/4.99G [00:02<00:26, 170MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   9% 461M/4.99G [00:02<00:26, 171MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  10% 482M/4.99G [00:02<00:26, 168MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  10% 503M/4.99G [00:02<00:26, 167MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  11% 524M/4.99G [00:02<00:26, 166MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  11% 545M/4.99G [00:03<00:26, 167MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  11% 566M/4.99G [00:03<00:26, 169MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  12% 587M/4.99G [00:03<00:26, 167MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  12% 608M/4.99G [00:03<00:29, 151MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  13% 629M/4.99G [00:03<00:26, 163MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  13% 650M/4.99G [00:03<00:26, 167MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  13% 671M/4.99G [00:03<00:25, 170MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  14% 692M/4.99G [00:03<00:24, 174MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  14% 713M/4.99G [00:04<00:26, 163MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  15% 734M/4.99G [00:04<00:28, 152MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  15% 755M/4.99G [00:04<00:26, 158MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  16% 776M/4.99G [00:04<00:25, 164MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  16% 797M/4.99G [00:04<00:25, 167MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  16% 818M/4.99G [00:04<00:24, 172MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  17% 839M/4.99G [00:04<00:28, 148MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  17% 860M/4.99G [00:05<00:27, 150MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  18% 881M/4.99G [00:07<02:15, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  18% 902M/4.99G [00:07<01:46, 38.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  18% 923M/4.99G [00:07<01:21, 49.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  19% 944M/4.99G [00:07<01:03, 63.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  19% 965M/4.99G [00:07<00:50, 79.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  20% 986M/4.99G [00:07<00:41, 96.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  20% 1.01G/4.99G [00:07<00:34, 114MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  21% 1.03G/4.99G [00:07<00:30, 131MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  21% 1.05G/4.99G [00:08<00:27, 146MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  21% 1.07G/4.99G [00:08<00:25, 152MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  22% 1.09G/4.99G [00:08<00:25, 152MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  22% 1.11G/4.99G [00:08<00:23, 163MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  23% 1.13G/4.99G [00:08<00:22, 170MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  23% 1.15G/4.99G [00:08<00:21, 177MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  24% 1.17G/4.99G [00:08<00:20, 183MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  24% 1.20G/4.99G [00:08<00:20, 185MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  24% 1.22G/4.99G [00:08<00:21, 177MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  25% 1.24G/4.99G [00:09<00:21, 171MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  25% 1.26G/4.99G [00:09<00:21, 170MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  26% 1.28G/4.99G [00:09<00:21, 170MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  26% 1.30G/4.99G [00:09<00:21, 170MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  26% 1.32G/4.99G [00:09<00:20, 177MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  27% 1.34G/4.99G [00:09<00:20, 182MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  27% 1.36G/4.99G [00:09<00:19, 186MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  28% 1.38G/4.99G [00:09<00:18, 190MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  28% 1.41G/4.99G [00:10<00:19, 180MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  29% 1.43G/4.99G [00:10<00:20, 176MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  29% 1.45G/4.99G [00:10<00:20, 174MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  29% 1.47G/4.99G [00:10<00:21, 168MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  30% 1.49G/4.99G [00:10<00:21, 162MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  30% 1.51G/4.99G [00:10<00:23, 150MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  31% 1.53G/4.99G [00:10<00:23, 147MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  31% 1.55G/4.99G [00:11<00:23, 144MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  32% 1.57G/4.99G [00:11<00:23, 144MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  32% 1.59G/4.99G [00:11<00:23, 143MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  32% 1.61G/4.99G [00:11<00:24, 140MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  33% 1.64G/4.99G [00:11<00:34, 96.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  33% 1.66G/4.99G [00:12<00:31, 105MB/s] \u001b[A\n",
            "model-00001-of-00003.safetensors:  34% 1.68G/4.99G [00:12<00:29, 113MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  34% 1.70G/4.99G [00:12<00:27, 120MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  34% 1.72G/4.99G [00:12<00:24, 131MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  35% 1.74G/4.99G [00:12<00:24, 135MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  35% 1.76G/4.99G [00:12<00:23, 136MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  36% 1.78G/4.99G [00:12<00:22, 142MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  36% 1.80G/4.99G [00:13<00:21, 149MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  37% 1.82G/4.99G [00:13<00:20, 156MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  37% 1.85G/4.99G [00:13<00:19, 159MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  37% 1.87G/4.99G [00:13<00:19, 163MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  38% 1.89G/4.99G [00:13<00:18, 165MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  38% 1.91G/4.99G [00:13<00:18, 167MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  39% 1.93G/4.99G [00:13<00:18, 168MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  39% 1.95G/4.99G [00:13<00:17, 170MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  39% 1.97G/4.99G [00:13<00:17, 174MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  40% 1.99G/4.99G [00:14<00:16, 179MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  40% 2.01G/4.99G [00:14<00:16, 179MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  41% 2.03G/4.99G [00:14<00:16, 179MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  41% 2.06G/4.99G [00:14<00:15, 186MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  42% 2.08G/4.99G [00:14<00:15, 189MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  42% 2.10G/4.99G [00:14<00:15, 193MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  42% 2.12G/4.99G [00:14<00:14, 195MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  43% 2.14G/4.99G [00:14<00:14, 192MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  43% 2.16G/4.99G [00:14<00:15, 179MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  44% 2.18G/4.99G [00:15<00:16, 173MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  44% 2.20G/4.99G [00:15<00:16, 171MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  45% 2.22G/4.99G [00:15<00:16, 171MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  45% 2.24G/4.99G [00:15<00:16, 170MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  45% 2.26G/4.99G [00:15<00:15, 178MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  46% 2.29G/4.99G [00:15<00:14, 183MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  46% 2.31G/4.99G [00:15<00:14, 187MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  47% 2.33G/4.99G [00:15<00:14, 190MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  47% 2.35G/4.99G [00:16<00:13, 192MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  47% 2.37G/4.99G [00:16<00:14, 187MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  48% 2.39G/4.99G [00:16<00:14, 176MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  48% 2.41G/4.99G [00:16<00:14, 176MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  49% 2.43G/4.99G [00:16<00:14, 177MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  49% 2.45G/4.99G [00:16<00:14, 176MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  50% 2.47G/4.99G [00:16<00:14, 179MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  50% 2.50G/4.99G [00:16<00:13, 181MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  50% 2.52G/4.99G [00:16<00:13, 185MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  51% 2.54G/4.99G [00:17<00:13, 181MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  51% 2.56G/4.99G [00:17<00:13, 181MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  52% 2.58G/4.99G [00:17<00:13, 180MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  52% 2.60G/4.99G [00:17<00:13, 182MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  53% 2.62G/4.99G [00:17<00:12, 186MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  53% 2.64G/4.99G [00:17<00:12, 189MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  53% 2.66G/4.99G [00:17<00:12, 193MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  54% 2.68G/4.99G [00:17<00:11, 193MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  54% 2.71G/4.99G [00:17<00:12, 182MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  55% 2.73G/4.99G [00:18<00:12, 178MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  55% 2.75G/4.99G [00:18<00:12, 178MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  55% 2.77G/4.99G [00:18<00:13, 171MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  56% 2.79G/4.99G [00:18<00:12, 172MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  56% 2.81G/4.99G [00:18<00:12, 171MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  57% 2.83G/4.99G [00:18<00:12, 171MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  57% 2.85G/4.99G [00:18<00:12, 174MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  58% 2.87G/4.99G [00:18<00:12, 173MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  58% 2.89G/4.99G [00:19<00:11, 175MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  58% 2.92G/4.99G [00:19<00:11, 180MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  59% 2.94G/4.99G [00:19<00:11, 183MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  59% 2.96G/4.99G [00:19<00:11, 182MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  60% 2.98G/4.99G [00:19<00:10, 187MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  60% 3.00G/4.99G [00:19<00:10, 186MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  60% 3.02G/4.99G [00:19<00:11, 179MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  61% 3.04G/4.99G [00:19<00:11, 176MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  61% 3.06G/4.99G [00:20<00:11, 174MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  62% 3.08G/4.99G [00:20<00:11, 173MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  62% 3.10G/4.99G [00:20<00:10, 179MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  63% 3.12G/4.99G [00:20<00:10, 184MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  63% 3.15G/4.99G [00:20<00:09, 186MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  63% 3.17G/4.99G [00:20<00:09, 186MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  64% 3.19G/4.99G [00:20<00:09, 190MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  64% 3.21G/4.99G [00:20<00:09, 182MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  65% 3.23G/4.99G [00:20<00:10, 176MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  65% 3.25G/4.99G [00:21<00:09, 174MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  66% 3.27G/4.99G [00:21<00:09, 174MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  66% 3.29G/4.99G [00:21<00:09, 174MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  66% 3.31G/4.99G [00:21<00:10, 168MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  67% 3.33G/4.99G [00:25<01:37, 17.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  67% 3.36G/4.99G [00:25<01:09, 23.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  68% 3.38G/4.99G [00:25<00:50, 31.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  68% 3.40G/4.99G [00:25<00:37, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  68% 3.42G/4.99G [00:25<00:28, 55.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  69% 3.44G/4.99G [00:25<00:22, 69.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  69% 3.46G/4.99G [00:25<00:17, 85.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  70% 3.48G/4.99G [00:26<00:19, 76.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  70% 3.50G/4.99G [00:26<00:16, 91.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  71% 3.52G/4.99G [00:26<00:13, 105MB/s] \u001b[A\n",
            "model-00001-of-00003.safetensors:  71% 3.54G/4.99G [00:26<00:12, 120MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  71% 3.57G/4.99G [00:26<00:10, 136MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  72% 3.59G/4.99G [00:26<00:09, 148MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  72% 3.61G/4.99G [00:26<00:08, 161MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  73% 3.63G/4.99G [00:27<00:07, 171MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  73% 3.65G/4.99G [00:27<00:07, 174MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  74% 3.67G/4.99G [00:27<00:07, 173MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  74% 3.69G/4.99G [00:27<00:07, 171MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  74% 3.71G/4.99G [00:27<00:07, 170MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  75% 3.73G/4.99G [00:27<00:07, 169MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  75% 3.75G/4.99G [00:27<00:07, 166MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  76% 3.77G/4.99G [00:27<00:07, 167MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  76% 3.80G/4.99G [00:28<00:07, 168MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  76% 3.82G/4.99G [00:28<00:06, 169MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  77% 3.84G/4.99G [00:28<00:06, 168MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  77% 3.86G/4.99G [00:28<00:06, 165MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  78% 3.88G/4.99G [00:28<00:06, 173MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  78% 3.90G/4.99G [00:28<00:06, 176MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  79% 3.92G/4.99G [00:28<00:05, 183MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  79% 3.94G/4.99G [00:28<00:05, 186MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  79% 3.96G/4.99G [00:28<00:05, 184MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  80% 3.98G/4.99G [00:29<00:05, 180MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  80% 4.01G/4.99G [00:29<00:05, 180MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  81% 4.03G/4.99G [00:32<00:51, 18.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  81% 4.05G/4.99G [00:33<00:39, 23.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  81% 4.07G/4.99G [00:33<00:29, 31.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  82% 4.09G/4.99G [00:33<00:21, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  82% 4.11G/4.99G [00:33<00:16, 54.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  83% 4.13G/4.99G [00:33<00:12, 69.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  83% 4.15G/4.99G [00:35<00:29, 28.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  84% 4.17G/4.99G [00:35<00:24, 33.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  84% 4.19G/4.99G [00:35<00:18, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  84% 4.22G/4.99G [00:35<00:14, 53.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  85% 4.24G/4.99G [00:36<00:11, 67.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  85% 4.26G/4.99G [00:36<00:08, 82.4MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  86% 4.28G/4.99G [00:36<00:07, 97.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  86% 4.30G/4.99G [00:36<00:06, 113MB/s] \u001b[A\n",
            "model-00001-of-00003.safetensors:  87% 4.32G/4.99G [00:36<00:05, 125MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  87% 4.34G/4.99G [00:36<00:07, 91.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  87% 4.36G/4.99G [00:37<00:05, 107MB/s] \u001b[A\n",
            "model-00001-of-00003.safetensors:  88% 4.38G/4.99G [00:37<00:05, 121MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  88% 4.40G/4.99G [00:37<00:04, 133MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  89% 4.42G/4.99G [00:37<00:03, 143MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  89% 4.45G/4.99G [00:37<00:03, 151MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  89% 4.47G/4.99G [00:37<00:03, 156MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  90% 4.49G/4.99G [00:37<00:03, 161MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  90% 4.51G/4.99G [00:37<00:02, 166MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  91% 4.53G/4.99G [00:38<00:02, 169MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  91% 4.55G/4.99G [00:38<00:02, 166MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  92% 4.57G/4.99G [00:38<00:02, 166MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  92% 4.59G/4.99G [00:38<00:02, 169MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  92% 4.61G/4.99G [00:38<00:02, 171MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  93% 4.63G/4.99G [00:38<00:02, 170MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  93% 4.66G/4.99G [00:38<00:01, 171MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  94% 4.68G/4.99G [00:38<00:01, 172MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  94% 4.70G/4.99G [00:39<00:01, 171MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  95% 4.72G/4.99G [00:39<00:01, 171MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  95% 4.74G/4.99G [00:39<00:01, 171MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  95% 4.76G/4.99G [00:39<00:01, 172MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  96% 4.78G/4.99G [00:39<00:01, 171MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  96% 4.80G/4.99G [00:39<00:01, 175MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  97% 4.82G/4.99G [00:39<00:00, 182MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  97% 4.84G/4.99G [00:39<00:00, 186MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  97% 4.87G/4.99G [00:39<00:00, 191MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  98% 4.89G/4.99G [00:40<00:00, 191MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  98% 4.91G/4.99G [00:40<00:00, 183MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  99% 4.93G/4.99G [00:40<00:00, 182MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  99% 4.95G/4.99G [00:40<00:00, 177MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors: 100% 4.97G/4.99G [00:40<00:00, 174MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors: 100% 4.99G/4.99G [00:40<00:00, 123MB/s]\n",
            "Downloading shards:  33% 1/3 [00:40<01:21, 40.77s/it]\n",
            "model-00002-of-00003.safetensors:   0% 0.00/4.96G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   0% 21.0M/4.96G [00:00<00:27, 180MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   1% 41.9M/4.96G [00:00<00:25, 190MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   1% 62.9M/4.96G [00:00<00:25, 194MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   2% 83.9M/4.96G [00:00<00:27, 178MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   2% 105M/4.96G [00:00<00:27, 179MB/s] \u001b[A\n",
            "model-00002-of-00003.safetensors:   3% 126M/4.96G [00:00<00:26, 184MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   3% 147M/4.96G [00:00<00:25, 190MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   3% 168M/4.96G [00:00<00:24, 194MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   4% 189M/4.96G [00:00<00:24, 197MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   4% 210M/4.96G [00:01<00:25, 184MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   5% 231M/4.96G [00:01<00:26, 181MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   5% 252M/4.96G [00:01<00:26, 178MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   5% 273M/4.96G [00:01<00:26, 176MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   6% 294M/4.96G [00:01<00:25, 180MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   6% 315M/4.96G [00:01<00:44, 104MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   7% 336M/4.96G [00:02<01:15, 61.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   7% 357M/4.96G [00:02<01:00, 76.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   8% 377M/4.96G [00:02<00:49, 91.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   8% 398M/4.96G [00:03<00:43, 106MB/s] \u001b[A\n",
            "model-00002-of-00003.safetensors:   8% 419M/4.96G [00:03<00:37, 120MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   9% 440M/4.96G [00:03<00:34, 132MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   9% 461M/4.96G [00:03<00:31, 141MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  10% 482M/4.96G [00:03<00:30, 149MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  10% 503M/4.96G [00:03<00:28, 155MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  11% 524M/4.96G [00:03<00:27, 160MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  11% 545M/4.96G [00:03<00:27, 162MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  11% 566M/4.96G [00:04<00:26, 166MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  12% 587M/4.96G [00:04<00:26, 167MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  12% 608M/4.96G [00:04<00:26, 166MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  13% 629M/4.96G [00:04<00:25, 168MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  13% 650M/4.96G [00:04<00:25, 169MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  14% 671M/4.96G [00:04<00:25, 171MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  14% 692M/4.96G [00:04<00:25, 170MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  14% 713M/4.96G [00:04<00:25, 170MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  15% 734M/4.96G [00:04<00:23, 177MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  15% 755M/4.96G [00:05<00:22, 184MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  16% 776M/4.96G [00:05<00:22, 187MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  16% 797M/4.96G [00:05<00:21, 192MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  16% 818M/4.96G [00:05<00:21, 194MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  17% 839M/4.96G [00:05<00:23, 179MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  17% 860M/4.96G [00:05<00:23, 174MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  18% 881M/4.96G [00:05<00:22, 181MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  18% 902M/4.96G [00:05<00:21, 184MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  19% 923M/4.96G [00:05<00:21, 184MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  19% 944M/4.96G [00:06<00:22, 178MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  19% 965M/4.96G [00:06<00:35, 113MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  20% 986M/4.96G [00:06<00:31, 126MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  20% 1.01G/4.96G [00:06<00:31, 126MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  21% 1.03G/4.96G [00:06<00:30, 127MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  21% 1.05G/4.96G [00:07<00:29, 132MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  22% 1.07G/4.96G [00:07<00:28, 135MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  22% 1.09G/4.96G [00:07<00:28, 138MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  22% 1.11G/4.96G [00:07<00:26, 144MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  23% 1.13G/4.96G [00:07<00:25, 151MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  23% 1.15G/4.96G [00:07<00:24, 157MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  24% 1.17G/4.96G [00:07<00:23, 158MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  24% 1.20G/4.96G [00:07<00:23, 162MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  25% 1.22G/4.96G [00:08<00:24, 156MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  25% 1.24G/4.96G [00:08<00:25, 149MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  25% 1.26G/4.96G [00:08<00:24, 150MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  26% 1.28G/4.96G [00:08<00:23, 155MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  26% 1.30G/4.96G [00:08<00:22, 162MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  27% 1.32G/4.96G [00:08<00:22, 164MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  27% 1.34G/4.96G [00:08<00:20, 172MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  27% 1.36G/4.96G [00:09<00:20, 179MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  28% 1.38G/4.96G [00:09<00:19, 183MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  28% 1.41G/4.96G [00:09<00:19, 184MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  29% 1.43G/4.96G [00:09<00:19, 177MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  29% 1.45G/4.96G [00:09<00:20, 175MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  30% 1.47G/4.96G [00:09<00:19, 176MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  30% 1.49G/4.96G [00:09<00:19, 174MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  30% 1.51G/4.96G [00:09<00:19, 174MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  31% 1.53G/4.96G [00:13<02:56, 19.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  31% 1.55G/4.96G [00:13<02:08, 26.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  32% 1.57G/4.96G [00:13<01:34, 35.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  32% 1.59G/4.96G [00:13<01:10, 47.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  33% 1.61G/4.96G [00:13<00:54, 61.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  33% 1.64G/4.96G [00:13<00:42, 77.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  33% 1.66G/4.96G [00:13<00:35, 92.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  34% 1.68G/4.96G [00:13<00:31, 105MB/s] \u001b[A\n",
            "model-00002-of-00003.safetensors:  34% 1.70G/4.96G [00:14<00:27, 118MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  35% 1.72G/4.96G [00:14<00:25, 128MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  35% 1.74G/4.96G [00:14<00:22, 141MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  36% 1.76G/4.96G [00:14<00:20, 153MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  36% 1.78G/4.96G [00:14<00:19, 162MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  36% 1.80G/4.96G [00:14<00:18, 173MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  37% 1.82G/4.96G [00:14<00:17, 180MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  37% 1.85G/4.96G [00:14<00:16, 184MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  38% 1.87G/4.96G [00:15<00:18, 167MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  38% 1.89G/4.96G [00:15<00:18, 167MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  38% 1.91G/4.96G [00:15<00:18, 169MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  39% 1.93G/4.96G [00:15<00:17, 170MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  39% 1.95G/4.96G [00:15<00:18, 164MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  40% 1.97G/4.96G [00:19<03:04, 16.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  40% 1.99G/4.96G [00:19<02:13, 22.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  41% 2.01G/4.96G [00:19<01:37, 30.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  41% 2.03G/4.96G [00:19<01:13, 40.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  41% 2.06G/4.96G [00:20<00:55, 51.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  42% 2.08G/4.96G [00:20<00:44, 65.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  42% 2.10G/4.96G [00:20<00:35, 80.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  43% 2.12G/4.96G [00:20<00:31, 91.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  43% 2.14G/4.96G [00:20<00:27, 104MB/s] \u001b[A\n",
            "model-00002-of-00003.safetensors:  44% 2.16G/4.96G [00:20<00:23, 118MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  44% 2.18G/4.96G [00:20<00:20, 134MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  44% 2.20G/4.96G [00:20<00:18, 149MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  45% 2.22G/4.96G [00:21<00:17, 161MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  45% 2.24G/4.96G [00:21<00:15, 170MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  46% 2.26G/4.96G [00:21<00:16, 166MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  46% 2.29G/4.96G [00:21<00:16, 164MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  47% 2.31G/4.96G [00:21<00:16, 165MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  47% 2.33G/4.96G [00:21<00:15, 168MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  47% 2.35G/4.96G [00:21<00:15, 165MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  48% 2.37G/4.96G [00:22<00:43, 60.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  48% 2.39G/4.96G [00:22<00:34, 74.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  49% 2.41G/4.96G [00:22<00:28, 89.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  49% 2.43G/4.96G [00:22<00:24, 104MB/s] \u001b[A\n",
            "model-00002-of-00003.safetensors:  49% 2.45G/4.96G [00:23<00:21, 119MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  50% 2.47G/4.96G [00:23<00:18, 131MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  50% 2.50G/4.96G [00:23<00:17, 140MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  51% 2.52G/4.96G [00:23<00:16, 147MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  51% 2.54G/4.96G [00:23<00:15, 155MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  52% 2.56G/4.96G [00:23<00:14, 164MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  52% 2.58G/4.96G [00:23<00:13, 172MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  52% 2.60G/4.96G [00:23<00:13, 179MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  53% 2.62G/4.96G [00:24<00:12, 184MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  53% 2.64G/4.96G [00:24<00:12, 189MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  54% 2.66G/4.96G [00:24<00:12, 183MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  54% 2.68G/4.96G [00:24<00:12, 178MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  55% 2.71G/4.96G [00:24<00:13, 173MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  55% 2.73G/4.96G [00:24<00:12, 173MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  55% 2.75G/4.96G [00:24<00:12, 172MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  56% 2.77G/4.96G [00:24<00:12, 173MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  56% 2.79G/4.96G [00:24<00:12, 177MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  57% 2.81G/4.96G [00:25<00:11, 180MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  57% 2.83G/4.96G [00:25<00:11, 185MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  58% 2.85G/4.96G [00:25<00:11, 188MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  58% 2.87G/4.96G [00:25<00:10, 192MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  58% 2.89G/4.96G [00:25<00:10, 191MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  59% 2.92G/4.96G [00:25<00:11, 173MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  59% 2.94G/4.96G [00:25<00:12, 165MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  60% 2.96G/4.96G [00:25<00:11, 168MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  60% 2.98G/4.96G [00:26<00:11, 168MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  60% 3.00G/4.96G [00:26<00:11, 169MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  61% 3.02G/4.96G [00:29<01:40, 19.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  61% 3.04G/4.96G [00:29<01:12, 26.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  62% 3.06G/4.96G [00:29<00:53, 35.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  62% 3.08G/4.96G [00:29<00:39, 47.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  63% 3.10G/4.96G [00:29<00:30, 61.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  63% 3.12G/4.96G [00:30<00:24, 74.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  63% 3.15G/4.96G [00:30<00:20, 88.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  64% 3.17G/4.96G [00:30<00:17, 104MB/s] \u001b[A\n",
            "model-00002-of-00003.safetensors:  64% 3.19G/4.96G [00:30<00:14, 119MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  65% 3.21G/4.96G [00:30<00:13, 131MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  65% 3.23G/4.96G [00:30<00:12, 142MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  66% 3.25G/4.96G [00:30<00:11, 152MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  66% 3.27G/4.96G [00:30<00:10, 156MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  66% 3.29G/4.96G [00:31<00:10, 162MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  67% 3.31G/4.96G [00:31<00:10, 154MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  67% 3.33G/4.96G [00:31<00:10, 151MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  68% 3.36G/4.96G [00:31<00:10, 152MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  68% 3.38G/4.96G [00:31<00:10, 150MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  69% 3.40G/4.96G [00:31<00:10, 149MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  69% 3.42G/4.96G [00:32<00:25, 60.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  69% 3.44G/4.96G [00:33<00:25, 58.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  70% 3.46G/4.96G [00:33<00:20, 73.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  70% 3.48G/4.96G [00:33<00:16, 88.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  71% 3.50G/4.96G [00:33<00:14, 103MB/s] \u001b[A\n",
            "model-00002-of-00003.safetensors:  71% 3.52G/4.96G [00:33<00:12, 117MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  71% 3.54G/4.96G [00:33<00:10, 129MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  72% 3.57G/4.96G [00:33<00:09, 143MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  72% 3.59G/4.96G [00:33<00:08, 155MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  73% 3.61G/4.96G [00:33<00:08, 166MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  73% 3.63G/4.96G [00:34<00:07, 168MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  74% 3.65G/4.96G [00:34<00:07, 174MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  74% 3.67G/4.96G [00:34<00:07, 165MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  74% 3.69G/4.96G [00:34<00:07, 165MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  75% 3.71G/4.96G [00:34<00:07, 166MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  75% 3.73G/4.96G [00:34<00:07, 169MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  76% 3.75G/4.96G [00:34<00:06, 174MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  76% 3.77G/4.96G [00:34<00:06, 178MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  77% 3.80G/4.96G [00:35<00:06, 183MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  77% 3.82G/4.96G [00:35<00:06, 187MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  77% 3.84G/4.96G [00:35<00:06, 181MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  78% 3.86G/4.96G [00:35<00:06, 183MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  78% 3.88G/4.96G [00:35<00:06, 176MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  79% 3.90G/4.96G [00:35<00:06, 175MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  79% 3.92G/4.96G [00:35<00:05, 175MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  80% 3.94G/4.96G [00:35<00:06, 169MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  80% 3.96G/4.96G [00:35<00:05, 169MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  80% 3.98G/4.96G [00:36<00:05, 168MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  81% 4.01G/4.96G [00:36<00:05, 174MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  81% 4.03G/4.96G [00:36<00:05, 172MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  82% 4.05G/4.96G [00:36<00:05, 171MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  82% 4.07G/4.96G [00:36<00:05, 170MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  82% 4.09G/4.96G [00:36<00:05, 169MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  83% 4.11G/4.96G [00:36<00:04, 171MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  83% 4.13G/4.96G [00:37<00:12, 65.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  84% 4.15G/4.96G [00:37<00:10, 80.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  84% 4.17G/4.96G [00:37<00:08, 94.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  85% 4.19G/4.96G [00:37<00:06, 110MB/s] \u001b[A\n",
            "model-00002-of-00003.safetensors:  85% 4.22G/4.96G [00:38<00:06, 121MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  85% 4.24G/4.96G [00:38<00:05, 132MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  86% 4.26G/4.96G [00:38<00:04, 141MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  86% 4.28G/4.96G [00:38<00:04, 147MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  87% 4.30G/4.96G [00:38<00:04, 153MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  87% 4.32G/4.96G [00:38<00:04, 158MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  88% 4.34G/4.96G [00:38<00:03, 161MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  88% 4.36G/4.96G [00:38<00:03, 164MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  88% 4.38G/4.96G [00:39<00:03, 161MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  89% 4.40G/4.96G [00:39<00:04, 130MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  89% 4.42G/4.96G [00:39<00:03, 140MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  90% 4.45G/4.96G [00:39<00:03, 147MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  90% 4.47G/4.96G [00:39<00:03, 156MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  91% 4.49G/4.96G [00:39<00:02, 162MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  91% 4.51G/4.96G [00:39<00:02, 164MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  91% 4.53G/4.96G [00:40<00:02, 165MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  92% 4.55G/4.96G [00:40<00:02, 166MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  92% 4.57G/4.96G [00:40<00:02, 171MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  93% 4.59G/4.96G [00:40<00:02, 177MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  93% 4.61G/4.96G [00:40<00:01, 182MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  93% 4.63G/4.96G [00:40<00:01, 184MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  94% 4.66G/4.96G [00:40<00:01, 186MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  94% 4.68G/4.96G [00:40<00:01, 181MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  95% 4.70G/4.96G [00:41<00:01, 175MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  95% 4.72G/4.96G [00:41<00:01, 173MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  96% 4.74G/4.96G [00:41<00:01, 173MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  96% 4.76G/4.96G [00:42<00:02, 68.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  96% 4.78G/4.96G [00:43<00:06, 27.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  97% 4.80G/4.96G [00:43<00:04, 36.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  97% 4.82G/4.96G [00:44<00:02, 47.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  98% 4.84G/4.96G [00:44<00:01, 61.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  98% 4.87G/4.96G [00:44<00:01, 75.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  99% 4.89G/4.96G [00:44<00:00, 87.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  99% 4.91G/4.96G [00:44<00:00, 98.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  99% 4.93G/4.96G [00:44<00:00, 107MB/s] \u001b[A\n",
            "model-00002-of-00003.safetensors: 100% 4.96G/4.96G [00:45<00:00, 110MB/s]\n",
            "Downloading shards:  67% 2/3 [01:25<00:43, 43.33s/it]\n",
            "model-00003-of-00003.safetensors:   0% 0.00/4.18G [00:00<?, ?B/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   0% 10.5M/4.18G [00:00<01:51, 37.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   1% 31.5M/4.18G [00:00<00:46, 89.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   1% 52.4M/4.18G [00:00<00:33, 125MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:   2% 73.4M/4.18G [00:00<00:27, 148MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   2% 94.4M/4.18G [00:00<00:24, 165MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   3% 115M/4.18G [00:00<00:24, 167MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:   3% 136M/4.18G [00:00<00:24, 164MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   4% 157M/4.18G [00:01<00:24, 164MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   4% 178M/4.18G [00:01<00:24, 164MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   5% 199M/4.18G [00:01<00:23, 168MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   5% 220M/4.18G [00:01<00:25, 156MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   6% 241M/4.18G [00:02<01:19, 49.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   6% 262M/4.18G [00:02<01:01, 63.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   7% 283M/4.18G [00:02<00:49, 78.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   7% 304M/4.18G [00:02<00:41, 94.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   8% 325M/4.18G [00:03<00:35, 109MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:   8% 346M/4.18G [00:03<00:31, 122MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   9% 367M/4.18G [00:03<00:28, 133MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   9% 388M/4.18G [00:03<00:26, 142MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  10% 409M/4.18G [00:03<00:25, 149MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  10% 430M/4.18G [00:03<00:24, 155MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  11% 451M/4.18G [00:03<00:23, 161MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  11% 472M/4.18G [00:03<00:22, 164MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  12% 493M/4.18G [00:04<00:22, 167MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  12% 514M/4.18G [00:04<00:21, 169MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  13% 535M/4.18G [00:04<00:21, 166MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  13% 556M/4.18G [00:04<00:21, 169MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  14% 577M/4.18G [00:04<00:20, 172MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  14% 598M/4.18G [00:04<00:20, 172MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  15% 619M/4.18G [00:04<00:20, 171MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  15% 640M/4.18G [00:04<00:20, 175MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  16% 661M/4.18G [00:04<00:19, 183MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  16% 682M/4.18G [00:05<00:18, 187MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  17% 703M/4.18G [00:05<00:18, 191MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  17% 724M/4.18G [00:05<00:18, 185MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  18% 744M/4.18G [00:05<00:19, 179MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  18% 765M/4.18G [00:05<00:18, 184MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  19% 786M/4.18G [00:05<00:18, 188MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  19% 807M/4.18G [00:05<00:17, 192MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  20% 828M/4.18G [00:05<00:17, 194MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  20% 849M/4.18G [00:05<00:17, 192MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  21% 870M/4.18G [00:06<00:18, 183MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  21% 891M/4.18G [00:06<00:18, 179MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  22% 912M/4.18G [00:06<00:18, 178MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  22% 933M/4.18G [00:06<00:17, 185MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  23% 954M/4.18G [00:06<00:17, 186MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  23% 975M/4.18G [00:06<00:16, 190MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  24% 996M/4.18G [00:06<00:16, 192MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  24% 1.02G/4.18G [00:06<00:16, 191MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  25% 1.04G/4.18G [00:07<00:17, 180MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  25% 1.06G/4.18G [00:07<00:17, 176MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  26% 1.08G/4.18G [00:07<00:17, 175MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  26% 1.10G/4.18G [00:07<00:17, 180MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  27% 1.12G/4.18G [00:07<00:17, 174MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  27% 1.14G/4.18G [00:07<00:17, 175MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  28% 1.16G/4.18G [00:07<00:16, 182MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  28% 1.18G/4.18G [00:07<00:16, 184MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  29% 1.21G/4.18G [00:07<00:15, 188MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  29% 1.23G/4.18G [00:08<00:15, 189MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  30% 1.25G/4.18G [00:08<00:16, 176MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  30% 1.27G/4.18G [00:08<00:16, 173MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  31% 1.29G/4.18G [00:08<00:17, 170MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  31% 1.31G/4.18G [00:08<00:16, 177MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  32% 1.33G/4.18G [00:08<00:15, 181MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  32% 1.35G/4.18G [00:08<00:15, 186MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  33% 1.37G/4.18G [00:08<00:14, 190MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  33% 1.39G/4.18G [00:08<00:14, 191MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  34% 1.42G/4.18G [00:09<00:15, 181MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  34% 1.44G/4.18G [00:09<00:16, 171MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  35% 1.46G/4.18G [00:09<00:15, 177MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  35% 1.48G/4.18G [00:09<00:15, 180MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  36% 1.50G/4.18G [00:09<00:14, 184MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  36% 1.52G/4.18G [00:09<00:14, 188MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  37% 1.54G/4.18G [00:09<00:14, 186MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  37% 1.56G/4.18G [00:09<00:15, 173MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  38% 1.58G/4.18G [00:10<00:15, 165MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  38% 1.60G/4.18G [00:10<00:15, 161MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  39% 1.63G/4.18G [00:10<00:15, 164MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  39% 1.65G/4.18G [00:10<00:15, 164MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  40% 1.67G/4.18G [00:10<00:15, 158MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  40% 1.69G/4.18G [00:10<00:16, 153MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  41% 1.71G/4.18G [00:10<00:15, 159MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  41% 1.73G/4.18G [00:11<00:15, 161MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  42% 1.75G/4.18G [00:11<00:14, 169MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  42% 1.77G/4.18G [00:11<00:14, 167MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  43% 1.79G/4.18G [00:11<00:15, 159MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  43% 1.81G/4.18G [00:11<00:15, 151MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  44% 1.84G/4.18G [00:11<00:15, 151MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  44% 1.86G/4.18G [00:11<00:15, 148MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  45% 1.88G/4.18G [00:11<00:15, 149MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  45% 1.90G/4.18G [00:15<01:52, 20.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  46% 1.92G/4.18G [00:15<01:21, 27.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  46% 1.94G/4.18G [00:15<00:59, 37.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  47% 1.96G/4.18G [00:15<00:44, 49.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  47% 1.98G/4.18G [00:15<00:34, 63.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  48% 2.00G/4.18G [00:15<00:27, 78.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  48% 2.02G/4.18G [00:15<00:23, 92.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  49% 2.04G/4.18G [00:15<00:20, 106MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:  49% 2.07G/4.18G [00:16<00:17, 119MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  50% 2.09G/4.18G [00:16<00:15, 134MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  50% 2.11G/4.18G [00:16<00:14, 147MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  51% 2.13G/4.18G [00:16<00:12, 158MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  51% 2.15G/4.18G [00:16<00:12, 165MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  52% 2.17G/4.18G [00:16<00:11, 173MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  52% 2.19G/4.18G [00:16<00:17, 111MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  53% 2.21G/4.18G [00:17<00:15, 124MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  53% 2.23G/4.18G [00:17<00:14, 134MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  54% 2.25G/4.18G [00:17<00:13, 144MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  54% 2.28G/4.18G [00:17<00:12, 153MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  55% 2.30G/4.18G [00:17<00:11, 163MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  55% 2.32G/4.18G [00:17<00:10, 172MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  56% 2.34G/4.18G [00:17<00:10, 174MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  56% 2.36G/4.18G [00:17<00:10, 180MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  57% 2.38G/4.18G [00:17<00:09, 184MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  57% 2.40G/4.18G [00:18<00:09, 188MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  58% 2.42G/4.18G [00:18<00:09, 176MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  59% 2.44G/4.18G [00:18<00:10, 171MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  59% 2.46G/4.18G [00:18<00:10, 170MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  60% 2.49G/4.18G [00:18<00:09, 175MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  60% 2.51G/4.18G [00:18<00:09, 182MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  61% 2.53G/4.18G [00:21<01:09, 23.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  61% 2.55G/4.18G [00:21<00:50, 32.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  62% 2.57G/4.18G [00:21<00:37, 43.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  62% 2.59G/4.18G [00:21<00:28, 56.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  63% 2.61G/4.18G [00:21<00:21, 71.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  63% 2.63G/4.18G [00:21<00:18, 85.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  64% 2.65G/4.18G [00:22<00:15, 100MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:  64% 2.67G/4.18G [00:22<00:13, 113MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  65% 2.69G/4.18G [00:22<00:11, 127MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  65% 2.72G/4.18G [00:22<00:10, 137MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  66% 2.74G/4.18G [00:22<00:09, 149MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  66% 2.76G/4.18G [00:22<00:09, 152MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  67% 2.78G/4.18G [00:22<00:08, 158MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  67% 2.80G/4.18G [00:22<00:08, 159MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  68% 2.82G/4.18G [00:23<00:08, 160MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  68% 2.84G/4.18G [00:23<00:08, 163MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  69% 2.86G/4.18G [00:23<00:07, 165MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  69% 2.88G/4.18G [00:23<00:08, 161MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  70% 2.90G/4.18G [00:23<00:08, 147MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  70% 2.93G/4.18G [00:23<00:08, 145MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  71% 2.95G/4.18G [00:23<00:08, 141MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  71% 2.97G/4.18G [00:24<00:08, 141MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  72% 2.99G/4.18G [00:24<00:16, 72.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  72% 3.01G/4.18G [00:24<00:13, 86.7MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  73% 3.03G/4.18G [00:24<00:11, 101MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:  73% 3.05G/4.18G [00:25<00:09, 114MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  74% 3.07G/4.18G [00:25<00:08, 125MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  74% 3.09G/4.18G [00:25<00:07, 138MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  75% 3.11G/4.18G [00:25<00:07, 152MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  75% 3.14G/4.18G [00:25<00:06, 163MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  76% 3.16G/4.18G [00:25<00:05, 171MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  76% 3.18G/4.18G [00:25<00:05, 174MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  77% 3.20G/4.18G [00:25<00:05, 175MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  77% 3.22G/4.18G [00:25<00:05, 174MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  78% 3.24G/4.18G [00:26<00:05, 172MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  78% 3.26G/4.18G [00:26<00:05, 172MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  79% 3.28G/4.18G [00:26<00:05, 175MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  79% 3.30G/4.18G [00:26<00:04, 178MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  80% 3.32G/4.18G [00:26<00:04, 184MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  80% 3.34G/4.18G [00:26<00:04, 184MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  81% 3.37G/4.18G [00:26<00:04, 172MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  81% 3.39G/4.18G [00:26<00:04, 171MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  82% 3.41G/4.18G [00:27<00:04, 168MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  82% 3.43G/4.18G [00:27<00:04, 174MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  83% 3.45G/4.18G [00:27<00:04, 181MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  83% 3.47G/4.18G [00:31<00:45, 15.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  84% 3.49G/4.18G [00:31<00:32, 21.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  84% 3.51G/4.18G [00:31<00:22, 29.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  85% 3.53G/4.18G [00:32<00:18, 34.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  85% 3.55G/4.18G [00:32<00:13, 46.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  86% 3.58G/4.18G [00:32<00:10, 58.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  86% 3.60G/4.18G [00:32<00:08, 71.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  87% 3.62G/4.18G [00:32<00:06, 86.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  87% 3.64G/4.18G [00:32<00:05, 103MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:  88% 3.66G/4.18G [00:32<00:04, 120MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  88% 3.68G/4.18G [00:32<00:03, 135MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  89% 3.70G/4.18G [00:33<00:03, 146MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  89% 3.72G/4.18G [00:33<00:02, 158MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  90% 3.74G/4.18G [00:33<00:02, 166MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  90% 3.76G/4.18G [00:33<00:02, 166MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  91% 3.79G/4.18G [00:33<00:02, 166MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  91% 3.81G/4.18G [00:33<00:02, 167MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  92% 3.83G/4.18G [00:33<00:01, 176MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  92% 3.85G/4.18G [00:33<00:01, 182MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  93% 3.87G/4.18G [00:35<00:09, 32.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  93% 3.89G/4.18G [00:35<00:06, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  94% 3.91G/4.18G [00:37<00:08, 30.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  94% 3.93G/4.18G [00:37<00:06, 39.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  95% 3.95G/4.18G [00:37<00:04, 51.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  95% 3.97G/4.18G [00:37<00:03, 64.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  96% 4.00G/4.18G [00:37<00:02, 79.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  96% 4.02G/4.18G [00:37<00:01, 94.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  97% 4.04G/4.18G [00:37<00:01, 111MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:  97% 4.06G/4.18G [00:37<00:00, 128MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  98% 4.08G/4.18G [00:38<00:00, 142MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  98% 4.10G/4.18G [00:38<00:00, 154MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  99% 4.12G/4.18G [00:38<00:00, 156MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  99% 4.14G/4.18G [00:38<00:00, 159MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors: 100% 4.18G/4.18G [00:38<00:00, 108MB/s]\n",
            "Downloading shards: 100% 3/3 [02:04<00:00, 41.52s/it]\n",
            "[INFO|modeling_utils.py:1519] 2024-06-03 07:53:11,385 >> Instantiating LlavaForConditionalGeneration model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:962] 2024-06-03 07:53:11,387 >> Generate config GenerationConfig {\n",
            "  \"pad_token_id\": 32001\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:962] 2024-06-03 07:53:11,907 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 3/3 [01:08<00:00, 22.90s/it]\n",
            "[INFO|modeling_utils.py:4280] 2024-06-03 07:54:20,823 >> All model checkpoint weights were used when initializing LlavaForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-06-03 07:54:20,823 >> All the weights of LlavaForConditionalGeneration were initialized from the model checkpoint at llava-hf/llava-1.5-7b-hf.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlavaForConditionalGeneration for predictions without further training.\n",
            "generation_config.json: 100% 141/141 [00:00<00:00, 943kB/s]\n",
            "[INFO|configuration_utils.py:917] 2024-06-03 07:54:20,952 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/generation_config.json\n",
            "[INFO|configuration_utils.py:962] 2024-06-03 07:54:20,952 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 32001\n",
            "}\n",
            "\n",
            "06/03/2024 07:54:20 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\n",
            "06/03/2024 07:54:20 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\n",
            "06/03/2024 07:54:20 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "06/03/2024 07:54:20 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "06/03/2024 07:54:21 - INFO - llamafactory.model.loader - trainable params: 4194304 || all params: 7067621376 || trainable%: 0.0593\n",
            "[INFO|trainer.py:641] 2024-06-03 07:54:21,920 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2078] 2024-06-03 07:54:22,285 >> ***** Running training *****\n",
            "[INFO|trainer.py:2079] 2024-06-03 07:54:22,285 >>   Num examples = 5\n",
            "[INFO|trainer.py:2080] 2024-06-03 07:54:22,285 >>   Num Epochs = 100\n",
            "[INFO|trainer.py:2081] 2024-06-03 07:54:22,285 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:2084] 2024-06-03 07:54:22,285 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2085] 2024-06-03 07:54:22,285 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2086] 2024-06-03 07:54:22,286 >>   Total optimization steps = 100\n",
            "[INFO|trainer.py:2087] 2024-06-03 07:54:22,288 >>   Number of trainable parameters = 4,194,304\n",
            "  0% 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "[WARNING|logging.py:329] 2024-06-03 07:54:24,700 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "{'loss': 0.7328, 'grad_norm': 0.3100017011165619, 'learning_rate': 9.759636527645633e-05, 'epoch': 6.4}\n",
            "{'loss': 0.5497, 'grad_norm': 0.15334869921207428, 'learning_rate': 9.05246588405146e-05, 'epoch': 13.0}\n",
            "{'loss': 0.3909, 'grad_norm': 0.39209219813346863, 'learning_rate': 7.947823644532198e-05, 'epoch': 20.0}\n",
            "{'loss': 0.224, 'grad_norm': 0.366840124130249, 'learning_rate': 6.554054685128856e-05, 'epoch': 27.0}\n",
            "{'loss': 0.1139, 'grad_norm': 0.23116442561149597, 'learning_rate': 5.007861840237924e-05, 'epoch': 33.6}\n",
            "{'loss': 0.0544, 'grad_norm': 0.24480417370796204, 'learning_rate': 3.460897894916863e-05, 'epoch': 40.0}\n",
            "{'loss': 0.0303, 'grad_norm': 0.03636083006858826, 'learning_rate': 2.0648912648459074e-05, 'epoch': 46.4}\n",
            "{'loss': 0.0179, 'grad_norm': 0.027192270383238792, 'learning_rate': 9.56764258331226e-06, 'epoch': 53.0}\n",
            "{'loss': 0.011, 'grad_norm': 0.1304520070552826, 'learning_rate': 2.452035422983734e-06, 'epoch': 60.0}\n",
            "{'loss': 0.0095, 'grad_norm': 0.1257449835538864, 'learning_rate': 0.0, 'epoch': 67.0}\n",
            "100% 100/100 [12:57<00:00,  8.65s/it][INFO|trainer.py:2329] 2024-06-03 08:07:20,187 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 777.8991, 'train_samples_per_second': 0.643, 'train_steps_per_second': 0.129, 'train_loss': 0.21343629635870456, 'epoch': 67.0}\n",
            "100% 100/100 [12:57<00:00,  7.78s/it]\n",
            "[INFO|trainer.py:3410] 2024-06-03 08:07:20,190 >> Saving model checkpoint to saves/llava1_5-7b/lora/sft\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:140: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:733] 2024-06-03 08:07:20,395 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-06-03 08:07:20,398 >> Model config LlavaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlavaForConditionalGeneration\"\n",
            "  ],\n",
            "  \"ignore_index\": -100,\n",
            "  \"image_token_index\": 32000,\n",
            "  \"model_type\": \"llava\",\n",
            "  \"pad_token_id\": 32001,\n",
            "  \"projector_hidden_act\": \"gelu\",\n",
            "  \"text_config\": {\n",
            "    \"_name_or_path\": \"lmsys/vicuna-7b-v1.5\",\n",
            "    \"architectures\": [\n",
            "      \"LlamaForCausalLM\"\n",
            "    ],\n",
            "    \"max_position_embeddings\": 4096,\n",
            "    \"model_type\": \"llama\",\n",
            "    \"rms_norm_eps\": 1e-05,\n",
            "    \"torch_dtype\": \"float16\",\n",
            "    \"vocab_size\": 32064\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.41.1\",\n",
            "  \"vision_config\": {\n",
            "    \"hidden_size\": 1024,\n",
            "    \"image_size\": 336,\n",
            "    \"intermediate_size\": 4096,\n",
            "    \"model_type\": \"clip_vision_model\",\n",
            "    \"num_attention_heads\": 16,\n",
            "    \"num_hidden_layers\": 24,\n",
            "    \"patch_size\": 14,\n",
            "    \"projection_dim\": 768,\n",
            "    \"vocab_size\": 32000\n",
            "  },\n",
            "  \"vision_feature_layer\": -2,\n",
            "  \"vision_feature_select_strategy\": \"default\"\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-06-03 08:07:20,444 >> tokenizer config file saved in saves/llava1_5-7b/lora/sft/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2522] 2024-06-03 08:07:20,444 >> Special tokens file saved in saves/llava1_5-7b/lora/sft/special_tokens_map.json\n",
            "[INFO|image_processing_utils.py:257] 2024-06-03 08:07:20,479 >> Image processor saved in saves/llava1_5-7b/lora/sft/preprocessor_config.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       67.0\n",
            "  total_flos               =  1599546GF\n",
            "  train_loss               =     0.2134\n",
            "  train_runtime            = 0:12:57.89\n",
            "  train_samples_per_second =      0.643\n",
            "  train_steps_per_second   =      0.129\n",
            "Figure saved at: saves/llava1_5-7b/lora/sft/training_loss.png\n",
            "06/03/2024 08:07:20 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.\n",
            "[INFO|trainer.py:3719] 2024-06-03 08:07:20,697 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-06-03 08:07:20,698 >>   Num examples = 1\n",
            "[INFO|trainer.py:3724] 2024-06-03 08:07:20,698 >>   Batch size = 1\n",
            "100% 1/1 [00:00<00:00, 475.49it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       67.0\n",
            "  eval_loss               =     2.0695\n",
            "  eval_runtime            = 0:00:00.88\n",
            "  eval_samples_per_second =      1.132\n",
            "  eval_steps_per_second   =      1.132\n",
            "[INFO|modelcard.py:450] 2024-06-03 08:07:21,584 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 llamafactory-cli train examples/lora_single_gpu/llava1_5_lora_sft.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 合并模型"
      ],
      "metadata": {
        "id": "nbyponddCk9C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VzB9srXllk8A",
        "outputId": "c05aa738-ca86-4c5f-ce5e-a6bef6066c5c",
        "colab": {
          "referenced_widgets": [
            "924a97a4cb3540e28fc5d66c357518ce",
            "49f964a1cc7449c9aa6ca946688af379",
            "a31ae64e9d5f48479618acac91e85a94",
            "6080811f78a542da8277bcf130b1a6d0",
            "bd6327eef3634dc799e8fc08239e65d8",
            "c6c4900b34bc4ee694fe927352270c4c",
            "4c1bfad92e534d3fbb1090a553eed14e",
            "a13be86043744b228adbca385b1039b7",
            "ffa2d5704fab405bb25b8904f54a84cb",
            "2d20f7e546bb43029604fad14800bf8e",
            "216313628de943d48b40c33312241c14"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:100: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "924a97a4cb3540e28fc5d66c357518ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "合并后的模型已保存到 saves/llava1_5-7b/lora/merge\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import LlavaForConditionalGeneration, AutoTokenizer\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "# 加载基础模型\n",
        "base_model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
        "base_model = LlavaForConditionalGeneration.from_pretrained(\n",
        "    base_model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        ").to(0)\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "# 加载LoRA微调的配置\n",
        "lora_model_dir = \"saves/llava1_5-7b/lora/sft\"\n",
        "peft_config = PeftConfig.from_pretrained(lora_model_dir)\n",
        "\n",
        "# 加载LoRA微调模型\n",
        "lora_model = PeftModel.from_pretrained(base_model, lora_model_dir)\n",
        "\n",
        "# 合并基础模型和LoRA权重\n",
        "merged_model = lora_model.merge_and_unload()\n",
        "\n",
        "# 保存合并后的模型\n",
        "merged_model_dir = \"saves/llava1_5-7b/lora/merge\"\n",
        "merged_model.save_pretrained(merged_model_dir)\n",
        "tokenizer.save_pretrained(merged_model_dir)\n",
        "\n",
        "print(f\"合并后的模型已保存到 {merged_model_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "import torch\n",
        "import gc\n",
        "torch.cuda.is_available()\n",
        "torch.cuda.empty_cache()\n",
        "# 显式释放 GPU 内存\n",
        "torch.cuda.empty_cache()\n",
        "# 手动调用垃圾回收\n",
        "gc.collect()\n",
        "!ps aux | grep python\n",
        "!kill 27851\n",
        "!nvidia-smi\n",
        "# !pwd\n",
        "# !cd LLaMA-Factory\n",
        "# !ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEKVu13tMupf",
        "outputId": "c76eea78-dad9-4d96-af6e-af7618559cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jun  3 09:34:56 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   75C    P0              34W /  70W |  13633MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n",
            "root          61  0.0  0.0      0     0 ?        Z    07:40   0:05 [python3] <defunct>\n",
            "root          62  0.0  0.3  69180 51408 ?        S    07:40   0:01 python3 /usr/local/bin/colab-file\n",
            "root         108  0.1  0.8 358724 108784 ?       Sl   07:40   0:09 /usr/bin/python3 /usr/local/bin/j\n",
            "root       27851  7.7 12.8 27454188 1709024 ?    Ssl  09:24   0:48 /usr/bin/python3 -m colab_kernel_\n",
            "root       27872  0.2  0.1 539408 14112 ?        Sl   09:24   0:01 /usr/bin/python3 /usr/local/lib/p\n",
            "root       30543  0.0  0.0   7376  3356 ?        S    09:34   0:00 /bin/bash -c ps aux | grep python\n",
            "root       30545  0.0  0.0   6484  2252 ?        S    09:34   0:00 grep python\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n_ecZKxlk8B"
      },
      "source": [
        "## 验证一下"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvCAQl3flk8B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "7b471d15541f47c6bb515e1fbe805f39",
            "24eba5ba5cb94766ac41886b1fe0f5f5",
            "40a28fb142cb4bb88a4b62ab35423c5f",
            "ecdaa4fd4e1643b1b49d97a9aa6ec619",
            "f5946beffe7b41bc815bbfb2a9378dc3",
            "f0c89fb9c0204cd5836593b991ba8529",
            "7f3a94c77447446f82388ace077a0e8d",
            "80b0f42a87a74499b625abe6117243c8",
            "42364e1f99714b509353befe09d9d0bf",
            "cc2395d70ac34b2cb9c5dc3447cab099",
            "d8a1be2740914d98b1245942c9d36504"
          ]
        },
        "outputId": "fdf35139-fb24-432f-ba37-8695c9d885b4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b471d15541f47c6bb515e1fbe805f39"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
        "\n",
        "# model_id = \"/root/LLaMA-Factory/saves/llava1_5-7b/lora/merge/\"\n",
        "model_id = \"/content/LLaMA-Factory/saves/llava1_5-7b/lora/merge\"\n",
        "\n",
        "prompt = \"USER: <image>\\nPlease describe this image\\nASSISTANT:\"\n",
        "# image_file = \"/root/LLaMA-Factory/data/mllm_demo_data/3.jpg\"\n",
        "image_file = \"/content/LLaMA-Factory/data/mllm_demo_data/3.jpg\"\n",
        "\n",
        "model = LlavaForConditionalGeneration.from_pretrained(\n",
        "    pretrained_model_name_or_path=model_id,\n",
        "    cache_dir=model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        ").to(0)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "\n",
        "raw_image = Image.open(image_file)\n",
        "inputs = processor(prompt, raw_image, return_tensors='pt').to(0, torch.float16)\n",
        "\n",
        "output = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
        "print(processor.decode(output[0][2:], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"USER: <image>\\n图片中是常铮？\\nASSISTANT:\"\n",
        "inputs = processor(prompt, raw_image, return_tensors='pt').to(0, torch.float16)\n",
        "output = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
        "print(processor.decode(output[0][2:], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNNaMxrYbEwG",
        "outputId": "13704e28-7522-4f15-f0ae-7665e235563d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ER:  \n",
            "图片中是常铮？\n",
            "ASSISTANT: 是的，常铮是中国宇航员。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert Pytorch Model to Quantize GGUF to Run on Ollama"
      ],
      "metadata": {
        "id": "IALNiFJfloF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cp -r [hugginface saved path] [new easy to remember path]/bonito\n",
        "\n",
        "# Import a model\n",
        "# git clone https://github.com/ollama/ollama.git\n",
        "# cd ollama\n",
        "# git submodule init\n",
        "# git submodule update llm/llama.cpp\n",
        "# python3 -m venv llm/llama.cpp/.venv\n",
        "# source llm/llama.cpp/.venv/bin/activate\n",
        "# pip install -r llm/llama.cpp/requirements.txt\n",
        "# make -C llm/llama.cpp quantize\n",
        "\n",
        "# python llm/llama.cpp/convert.py --vocab-type bpe \\\n",
        "#                                 ./model/Llama3-8B-Huatuo-SFT \\\n",
        "#                                 --outtype f16 \\\n",
        "#                                 --outfile ./model/llama3-8b-huatuo-sft-f16.bin\n",
        "# llm/llama.cpp/quantize ./model/llama3-8b-huatuo-sft-f16.bin ./model/llama3-8b-huatuo-sft-q4.bin q4_0\n",
        "# ollama create llama3-8b-huatuo-sft -f ./model/llama3-8b-huatuo-sft-q4.Modelfile\n",
        "\n",
        "\n",
        "git clone https://github.com/ollama/ollama.git\n",
        "cd ollama\n",
        "git submodule init\n",
        "git submodule update llm/llama.cpp\n",
        "pip install -r llm/llama.cpp/requirements.txt\n",
        "\n",
        "\n",
        "# python llama.cpp/convert.py bonito\n",
        "\n",
        "# cd llama.cpp\n",
        "# make\n",
        "# ./quantize [your path to]/bonito/ggml-model-f32.gguf [your path to]/bonito/ggml-model-f32.gguf-Q4_K_M.gguf Q4_K_M"
      ],
      "metadata": {
        "id": "YUK0W5VOlrAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CSqAmvqf4wFJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "bVSTm0gNlk76",
        "6ePTx-QlC_sx",
        "IALNiFJfloF2"
      ]
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "924a97a4cb3540e28fc5d66c357518ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_49f964a1cc7449c9aa6ca946688af379",
              "IPY_MODEL_a31ae64e9d5f48479618acac91e85a94",
              "IPY_MODEL_6080811f78a542da8277bcf130b1a6d0"
            ],
            "layout": "IPY_MODEL_bd6327eef3634dc799e8fc08239e65d8"
          }
        },
        "49f964a1cc7449c9aa6ca946688af379": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6c4900b34bc4ee694fe927352270c4c",
            "placeholder": "​",
            "style": "IPY_MODEL_4c1bfad92e534d3fbb1090a553eed14e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a31ae64e9d5f48479618acac91e85a94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a13be86043744b228adbca385b1039b7",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ffa2d5704fab405bb25b8904f54a84cb",
            "value": 3
          }
        },
        "6080811f78a542da8277bcf130b1a6d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d20f7e546bb43029604fad14800bf8e",
            "placeholder": "​",
            "style": "IPY_MODEL_216313628de943d48b40c33312241c14",
            "value": " 3/3 [00:01&lt;00:00,  2.30it/s]"
          }
        },
        "bd6327eef3634dc799e8fc08239e65d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6c4900b34bc4ee694fe927352270c4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c1bfad92e534d3fbb1090a553eed14e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a13be86043744b228adbca385b1039b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffa2d5704fab405bb25b8904f54a84cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d20f7e546bb43029604fad14800bf8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "216313628de943d48b40c33312241c14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b471d15541f47c6bb515e1fbe805f39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24eba5ba5cb94766ac41886b1fe0f5f5",
              "IPY_MODEL_40a28fb142cb4bb88a4b62ab35423c5f",
              "IPY_MODEL_ecdaa4fd4e1643b1b49d97a9aa6ec619"
            ],
            "layout": "IPY_MODEL_f5946beffe7b41bc815bbfb2a9378dc3"
          }
        },
        "24eba5ba5cb94766ac41886b1fe0f5f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0c89fb9c0204cd5836593b991ba8529",
            "placeholder": "​",
            "style": "IPY_MODEL_7f3a94c77447446f82388ace077a0e8d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "40a28fb142cb4bb88a4b62ab35423c5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80b0f42a87a74499b625abe6117243c8",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_42364e1f99714b509353befe09d9d0bf",
            "value": 3
          }
        },
        "ecdaa4fd4e1643b1b49d97a9aa6ec619": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc2395d70ac34b2cb9c5dc3447cab099",
            "placeholder": "​",
            "style": "IPY_MODEL_d8a1be2740914d98b1245942c9d36504",
            "value": " 3/3 [00:01&lt;00:00,  2.56it/s]"
          }
        },
        "f5946beffe7b41bc815bbfb2a9378dc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0c89fb9c0204cd5836593b991ba8529": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f3a94c77447446f82388ace077a0e8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80b0f42a87a74499b625abe6117243c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42364e1f99714b509353befe09d9d0bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc2395d70ac34b2cb9c5dc3447cab099": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8a1be2740914d98b1245942c9d36504": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}