{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbf09908-baaa-4662-ab91-f2793d9da1d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     /root/anaconda3\n",
      "huggingface           *  /root/anaconda3/envs/huggingface\n",
      "\n",
      "declare -x CLICOLOR=\"1\"\n",
      "declare -x CLICOLOR_FORCE=\"1\"\n",
      "declare -x CONDA_DEFAULT_ENV=\"huggingface\"\n",
      "declare -x CONDA_EXE=\"/root/anaconda3/bin/conda\"\n",
      "declare -x CONDA_PREFIX=\"/root/anaconda3/envs/huggingface\"\n",
      "declare -x CONDA_PREFIX_1=\"/root/anaconda3\"\n",
      "declare -x CONDA_PROMPT_MODIFIER=\"(huggingface) \"\n",
      "declare -x CONDA_PYTHON_EXE=\"/root/anaconda3/bin/python\"\n",
      "declare -x CONDA_SHLVL=\"2\"\n",
      "declare -x DBUS_SESSION_BUS_ADDRESS=\"unix:path=/run/user/0/bus\"\n",
      "declare -x DISPLAY=\"localhost:10.0\"\n",
      "declare -x FORCE_COLOR=\"1\"\n",
      "declare -x GIT_PAGER=\"cat\"\n",
      "declare -x GSETTINGS_SCHEMA_DIR=\"/root/anaconda3/envs/huggingface/share/glib-2.0/schemas\"\n",
      "declare -x GSETTINGS_SCHEMA_DIR_CONDA_BACKUP=\"\"\n",
      "declare -x HOME=\"/root\"\n",
      "declare -x JPY_PARENT_PID=\"2675\"\n",
      "declare -x JPY_SESSION_NAME=\"/root/Untitled.ipynb\"\n",
      "declare -x LANG=\"en_US.UTF-8\"\n",
      "declare -x LESSCLOSE=\"/usr/bin/lesspipe %s %s\"\n",
      "declare -x LESSOPEN=\"| /usr/bin/lesspipe %s\"\n",
      "declare -x LOGNAME=\"root\"\n",
      "declare -x LS_COLORS=\"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.webp=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:\"\n",
      "declare -x MOTD_SHOWN=\"pam\"\n",
      "declare -x MPLBACKEND=\"module://matplotlib_inline.backend_inline\"\n",
      "declare -x OLDPWD\n",
      "declare -x PAGER=\"cat\"\n",
      "declare -x PATH=\"/root/anaconda3/envs/huggingface/bin:/root/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\"\n",
      "declare -x PWD=\"/root\"\n",
      "declare -x PYDEVD_USE_FRAME_EVAL=\"NO\"\n",
      "declare -x SHELL=\"/bin/bash\"\n",
      "declare -x SHLVL=\"3\"\n",
      "declare -x SSH_CLIENT=\"10.0.0.18 52917 22\"\n",
      "declare -x SSH_CONNECTION=\"10.0.0.18 52917 10.0.0.17 22\"\n",
      "declare -x SSH_TTY=\"/dev/pts/0\"\n",
      "declare -x TERM=\"xterm-color\"\n",
      "declare -x TERM_PROGRAM=\"tmux\"\n",
      "declare -x TERM_PROGRAM_VERSION=\"3.2a\"\n",
      "declare -x TMUX=\"/tmp/tmux-0/default,2626,0\"\n",
      "declare -x TMUX_PANE=\"%0\"\n",
      "declare -x USER=\"root\"\n",
      "declare -x XDG_DATA_DIRS=\"/usr/local/share:/usr/share:/var/lib/snapd/desktop\"\n",
      "declare -x XDG_RUNTIME_DIR=\"/run/user/0\"\n",
      "declare -x XDG_SESSION_CLASS=\"user\"\n",
      "declare -x XDG_SESSION_ID=\"1\"\n",
      "declare -x XDG_SESSION_TYPE=\"tty\"\n",
      "declare -x _=\"/root/anaconda3/envs/huggingface/bin/jupyter-lab\"\n",
      "declare -x _CE_CONDA=\"\"\n",
      "declare -x _CE_M=\"\"\n",
      "declare -x http_proxy=\"http://10.0.0.16:8080/\"\n",
      "declare -x https_proxy=\"http://10.0.0.16:8080/\"\n",
      "declare -x no_proxy=\"localhost,127.0.0.0/8,10.10.0.0/16,10.18.0.0/16,cluster.local,.svc\"\n"
     ]
    }
   ],
   "source": [
    "!conda info --envs\n",
    "!export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25081477-2006-4539-9294-207adf43f999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in as: changzheng\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, HfFolder\n",
    "\n",
    "# 替换为你的 Hugging Face 用户令牌\n",
    "token = \"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\"\n",
    "\n",
    "# 保存令牌\n",
    "HfFolder.save_token(token)\n",
    "\n",
    "# 创建 HfApi 实例\n",
    "api = HfApi()\n",
    "\n",
    "# 验证登录是否成功\n",
    "user_info = api.whoami()\n",
    "print(\"Successfully logged in as:\", user_info[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c896b0d1-2c56-4427-bf2e-dd02623e4a1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaMA-Factory'...\n",
      "remote: Enumerating objects: 259, done.\u001b[K\n",
      "remote: Counting objects: 100% (259/259), done.\u001b[K\n",
      "remote: Compressing objects: 100% (220/220), done.\u001b[K\n",
      "remote: Total 259 (delta 45), reused 141 (delta 28), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (259/259), 7.77 MiB | 1017.00 KiB/s, done.\n",
      "Resolving deltas: 100% (45/45), done.\n",
      "/root/LLaMA-Factory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/huggingface/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///root/LLaMA-Factory\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers>=4.37.2 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from llamafactory==0.7.2.dev0) (4.41.1)\n",
      "Requirement already satisfied: datasets>=2.14.3 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from llamafactory==0.7.2.dev0) (2.18.0)\n",
      "Requirement already satisfied: accelerate>=0.27.2 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from llamafactory==0.7.2.dev0) (0.30.1)\n",
      "Requirement already satisfied: peft>=0.10.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from llamafactory==0.7.2.dev0) (0.11.1)\n",
      "Requirement already satisfied: trl>=0.8.1 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from llamafactory==0.7.2.dev0) (0.8.6)\n",
      "Requirement already satisfied: gradio>=4.0.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from llamafactory==0.7.2.dev0) (4.31.5)\n",
      "Requirement already satisfied: scipy in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from llamafactory==0.7.2.dev0) (1.13.1)\n",
      "Requirement already satisfied: einops in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from llamafactory==0.7.2.dev0) (0.8.0)\n",
      "Requirement already satisfied: sentencepiece in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from llamafactory==0.7.2.dev0) (0.2.0)\n",
      "Requirement already satisfied: protobuf in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from llamafactory==0.7.2.dev0) (4.25.3)\n",
      "Requirement already satisfied: uvicorn in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from llamafactory==0.7.2.dev0) (0.29.0)\n",
      "Requirement already satisfied: pydantic in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from llamafactory==0.7.2.dev0) (2.7.1)\n",
      "Requirement already satisfied: fastapi in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from llamafactory==0.7.2.dev0) (0.111.0)\n",
      "Requirement already satisfied: sse-starlette in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from llamafactory==0.7.2.dev0) (2.1.0)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from llamafactory==0.7.2.dev0) (3.9.0)\n",
      "Requirement already satisfied: fire in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from llamafactory==0.7.2.dev0) (0.6.0)\n",
      "Requirement already satisfied: packaging in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from llamafactory==0.7.2.dev0) (23.2)\n",
      "Requirement already satisfied: pyyaml in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from llamafactory==0.7.2.dev0) (6.0.1)\n",
      "Requirement already satisfied: bitsandbytes>=0.39.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from llamafactory==0.7.2.dev0) (0.43.1)\n",
      "Requirement already satisfied: torch>=1.13.1 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from llamafactory==0.7.2.dev0) (2.1.2)\n",
      "Requirement already satisfied: nltk in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from llamafactory==0.7.2.dev0) (3.8.1)\n",
      "Requirement already satisfied: jieba in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from llamafactory==0.7.2.dev0) (0.42.1)\n",
      "Requirement already satisfied: rouge-chinese in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from llamafactory==0.7.2.dev0) (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from accelerate>=0.27.2->llamafactory==0.7.2.dev0) (1.24.4)\n",
      "Requirement already satisfied: psutil in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from accelerate>=0.27.2->llamafactory==0.7.2.dev0) (5.9.0)\n",
      "Requirement already satisfied: huggingface-hub in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from accelerate>=0.27.2->llamafactory==0.7.2.dev0) (0.23.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from accelerate>=0.27.2->llamafactory==0.7.2.dev0) (0.4.3)\n",
      "Requirement already satisfied: filelock in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (3.14.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets>=2.14.3->llamafactory==0.7.2.dev0) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (3.9.5)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (5.3.0)\n",
      "Requirement already satisfied: ffmpy in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.3.2)\n",
      "Requirement already satisfied: gradio-client==0.16.4 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.16.4)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.27.0)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (6.4.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (3.1.3)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (2.1.3)\n",
      "Requirement already satisfied: orjson~=3.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (3.10.3)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (10.3.0)\n",
      "Requirement already satisfied: pydub in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.0.9)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.4.5)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.12.3)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (4.11.0)\n",
      "Requirement already satisfied: urllib3~=2.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (2.2.1)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from gradio-client==0.16.4->gradio>=4.0.0->llamafactory==0.7.2.dev0) (11.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (4.52.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (2.9.0.post0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from pydantic->llamafactory==0.7.2.dev0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from pydantic->llamafactory==0.7.2.dev0) (2.18.2)\n",
      "Requirement already satisfied: sympy in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.7.2.dev0) (1.12)\n",
      "Requirement already satisfied: networkx in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.7.2.dev0) (3.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.7.2.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.7.2.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.7.2.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.7.2.dev0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.7.2.dev0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.7.2.dev0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.7.2.dev0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.7.2.dev0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.7.2.dev0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.7.2.dev0) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.7.2.dev0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.7.2.dev0) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->llamafactory==0.7.2.dev0) (12.5.40)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from transformers>=4.37.2->llamafactory==0.7.2.dev0) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from transformers>=4.37.2->llamafactory==0.7.2.dev0) (0.19.1)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from trl>=0.8.1->llamafactory==0.7.2.dev0) (0.8.4)\n",
      "Requirement already satisfied: click>=7.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from uvicorn->llamafactory==0.7.2.dev0) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from uvicorn->llamafactory==0.7.2.dev0) (0.14.0)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from fastapi->llamafactory==0.7.2.dev0) (0.37.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from fastapi->llamafactory==0.7.2.dev0) (0.0.4)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from fastapi->llamafactory==0.7.2.dev0) (5.10.0)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from fastapi->llamafactory==0.7.2.dev0) (2.1.1)\n",
      "Requirement already satisfied: six in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from fire->llamafactory==0.7.2.dev0) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from fire->llamafactory==0.7.2.dev0) (2.4.0)\n",
      "Requirement already satisfied: joblib in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from nltk->llamafactory==0.7.2.dev0) (1.4.2)\n",
      "Requirement already satisfied: anyio in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from sse-starlette->llamafactory==0.7.2.dev0) (4.2.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (4.19.2)\n",
      "Requirement already satisfied: toolz in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.12.1)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->llamafactory==0.7.2.dev0) (2.6.1)\n",
      "Requirement already satisfied: idna>=2.0.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->llamafactory==0.7.2.dev0) (3.7)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (4.0.3)\n",
      "Requirement already satisfied: certifi in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.7.2.dev0) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.7.2.dev0) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.7.2.dev0) (1.3.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from pandas->datasets>=2.14.3->llamafactory==0.7.2.dev0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from pandas->datasets>=2.14.3->llamafactory==0.7.2.dev0) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.14.3->llamafactory==0.7.2.dev0) (2.0.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from anyio->sse-starlette->llamafactory==0.7.2.dev0) (1.2.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.7.2.dev0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.7.2.dev0) (13.7.1)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.8.1->llamafactory==0.7.2.dev0) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.8.1->llamafactory==0.7.2.dev0) (1.7.1)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.7.2.dev0) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.7.2.dev0) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.7.2.dev0) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.7.2.dev0) (0.21.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from sympy->torch>=1.13.1->llamafactory==0.7.2.dev0) (1.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.10.6)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.7.2.dev0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.7.2.dev0) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/anaconda3/envs/huggingface/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.1.1)\n",
      "Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: llamafactory\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.7.2.dev0-0.editable-py3-none-any.whl size=18704 sha256=60e3d9073b45f22957daa9babcc069992fd940c7c87ed24761bf97912f6ee8c9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-s9ul251y/wheels/b9/b4/a8/6ecb95f1aaa772330e7f82ffab08e1d3e480fe77976c8ec250\n",
      "Successfully built llamafactory\n",
      "Installing collected packages: llamafactory\n",
      "  Attempting uninstall: llamafactory\n",
      "    Found existing installation: llamafactory 0.7.2.dev0\n",
      "    Uninstalling llamafactory-0.7.2.dev0:\n",
      "      Successfully uninstalled llamafactory-0.7.2.dev0\n",
      "Successfully installed llamafactory-0.7.2.dev0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
    "%cd LLaMA-Factory\n",
    "%pip install -e .[torch,metrics,bitsandbytes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7dad8ff-f2fe-4f1d-96ca-8e1b1c7983ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 环境满足\n",
      "Sat Jun  1 14:47:49 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 Ti     Off |   00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   44C    P8              9W /  165W |       5MiB /  16380MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "try:\n",
    "  assert torch.cuda.is_available() is True\n",
    "  print(\"GPU 环境满足\")\n",
    "  !nvidia-smi\n",
    "except AssertionError:\n",
    "  print(\"需要 GPU 环境\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95f33f68-3cf3-4832-b75e-cab8f7598220",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/huggingface/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "06/01/2024 14:48:07 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-01 14:48:08,551 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-01 14:48:08,552 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-01 14:48:08,552 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-01 14:48:08,552 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-01 14:48:08,552 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/tokenizer_config.json\n",
      "[WARNING|logging.py:314] 2024-06-01 14:48:08,652 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|image_processing_utils.py:374] 2024-06-01 14:48:10,011 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/preprocessor_config.json\n",
      "[INFO|image_processing_utils.py:374] 2024-06-01 14:48:10,417 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/preprocessor_config.json\n",
      "[INFO|image_processing_utils.py:424] 2024-06-01 14:48:10,420 >> Image processor CLIPImageProcessor {\n",
      "  \"_valid_processor_keys\": [\n",
      "    \"images\",\n",
      "    \"do_resize\",\n",
      "    \"size\",\n",
      "    \"resample\",\n",
      "    \"do_center_crop\",\n",
      "    \"crop_size\",\n",
      "    \"do_rescale\",\n",
      "    \"rescale_factor\",\n",
      "    \"do_normalize\",\n",
      "    \"image_mean\",\n",
      "    \"image_std\",\n",
      "    \"do_convert_rgb\",\n",
      "    \"return_tensors\",\n",
      "    \"data_format\",\n",
      "    \"input_data_format\"\n",
      "  ],\n",
      "  \"crop_size\": {\n",
      "    \"height\": 336,\n",
      "    \"width\": 336\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"processor_class\": \"LlavaProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 336\n",
      "  }\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-01 14:48:10,828 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-01 14:48:10,828 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-01 14:48:10,828 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-01 14:48:10,828 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-01 14:48:10,828 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/tokenizer_config.json\n",
      "[WARNING|logging.py:314] 2024-06-01 14:48:10,894 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|processing_utils.py:400] 2024-06-01 14:48:11,309 >> Processor LlavaProcessor:\n",
      "- image_processor: CLIPImageProcessor {\n",
      "  \"_valid_processor_keys\": [\n",
      "    \"images\",\n",
      "    \"do_resize\",\n",
      "    \"size\",\n",
      "    \"resample\",\n",
      "    \"do_center_crop\",\n",
      "    \"crop_size\",\n",
      "    \"do_rescale\",\n",
      "    \"rescale_factor\",\n",
      "    \"do_normalize\",\n",
      "    \"image_mean\",\n",
      "    \"image_std\",\n",
      "    \"do_convert_rgb\",\n",
      "    \"return_tensors\",\n",
      "    \"data_format\",\n",
      "    \"input_data_format\"\n",
      "  ],\n",
      "  \"crop_size\": {\n",
      "    \"height\": 336,\n",
      "    \"width\": 336\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"processor_class\": \"LlavaProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 336\n",
      "  }\n",
      "}\n",
      "\n",
      "- tokenizer: LlamaTokenizerFast(name_or_path='llava-hf/llava-1.5-7b-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32000: AddedToken(\"<image>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32001: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "\n",
      "{\n",
      "  \"processor_class\": \"LlavaProcessor\"\n",
      "}\n",
      "\n",
      "06/01/2024 14:48:11 - INFO - llamafactory.data.loader - Loading dataset mllm_demo.json...\n",
      "Generating train split: 6 examples [00:00, 158.43 examples/s]\n",
      "num_proc must be <= 6. Reducing num_proc to 6 for dataset of size 6.\n",
      "Converting format of dataset (num_proc=6): 100%|█| 6/6 [00:00<00:00, 50.87 examp\n",
      "num_proc must be <= 6. Reducing num_proc to 6 for dataset of size 6.\n",
      "Running tokenizer on dataset (num_proc=6): 100%|█| 6/6 [00:00<00:00, 17.43 examp\n",
      "input_ids:\n",
      "[319, 13563, 1546, 263, 12758, 1404, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 1404, 29915, 29879, 5155, 29889, 3148, 1001, 29901, 29871, 32000, 11644, 526, 896, 29973, 319, 1799, 9047, 13566, 29901, 2688, 29915, 276, 476, 1662, 322, 402, 2267, 29920, 1335, 515, 19584, 13564, 436, 29889, 2, 3148, 1001, 29901, 1724, 526, 896, 2599, 29973, 319, 1799, 9047, 13566, 29901, 2688, 526, 10894, 1218, 373, 278, 269, 11953, 1746, 29889, 2]\n",
      "inputs:\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image> Who are they? ASSISTANT: They're Kane and Gretzka from Bayern Munich.</s> USER: What are they doing? ASSISTANT: They are celebrating on the soccer field.</s>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2688, 29915, 276, 476, 1662, 322, 402, 2267, 29920, 1335, 515, 19584, 13564, 436, 29889, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2688, 526, 10894, 1218, 373, 278, 269, 11953, 1746, 29889, 2]\n",
      "labels:\n",
      "They're Kane and Gretzka from Bayern Munich.</s> They are celebrating on the soccer field.</s>\n",
      "/root/anaconda3/envs/huggingface/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-01 14:48:16,148 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/config.json\n",
      "/root/anaconda3/envs/huggingface/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:100: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:796] 2024-06-01 14:48:16,158 >> Model config LlavaConfig {\n",
      "  \"_name_or_path\": \"llava-hf/llava-1.5-7b-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlavaForConditionalGeneration\"\n",
      "  ],\n",
      "  \"ignore_index\": -100,\n",
      "  \"image_token_index\": 32000,\n",
      "  \"model_type\": \"llava\",\n",
      "  \"pad_token_id\": 32001,\n",
      "  \"projector_hidden_act\": \"gelu\",\n",
      "  \"text_config\": {\n",
      "    \"_name_or_path\": \"lmsys/vicuna-7b-v1.5\",\n",
      "    \"architectures\": [\n",
      "      \"LlamaForCausalLM\"\n",
      "    ],\n",
      "    \"max_position_embeddings\": 4096,\n",
      "    \"model_type\": \"llama\",\n",
      "    \"rms_norm_eps\": 1e-05,\n",
      "    \"torch_dtype\": \"float16\",\n",
      "    \"vocab_size\": 32064\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.41.1\",\n",
      "  \"vision_config\": {\n",
      "    \"hidden_size\": 1024,\n",
      "    \"image_size\": 336,\n",
      "    \"intermediate_size\": 4096,\n",
      "    \"model_type\": \"clip_vision_model\",\n",
      "    \"num_attention_heads\": 16,\n",
      "    \"num_hidden_layers\": 24,\n",
      "    \"patch_size\": 14,\n",
      "    \"projection_dim\": 768,\n",
      "    \"vocab_size\": 32000\n",
      "  },\n",
      "  \"vision_feature_layer\": -2,\n",
      "  \"vision_feature_select_strategy\": \"default\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3474] 2024-06-01 14:48:16,166 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1519] 2024-06-01 14:48:16,168 >> Instantiating LlavaForConditionalGeneration model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:962] 2024-06-01 14:48:16,169 >> Generate config GenerationConfig {\n",
      "  \"pad_token_id\": 32001\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:962] 2024-06-01 14:48:16,310 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:33<00:00, 11.29s/it]\n",
      "[INFO|modeling_utils.py:4280] 2024-06-01 14:48:50,288 >> All model checkpoint weights were used when initializing LlavaForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:4288] 2024-06-01 14:48:50,288 >> All the weights of LlavaForConditionalGeneration were initialized from the model checkpoint at llava-hf/llava-1.5-7b-hf.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlavaForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:917] 2024-06-01 14:48:50,765 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-06-01 14:48:50,766 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 32001\n",
      "}\n",
      "\n",
      "06/01/2024 14:48:50 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\n",
      "06/01/2024 14:48:50 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\n",
      "06/01/2024 14:48:50 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "06/01/2024 14:48:50 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "06/01/2024 14:48:52 - INFO - llamafactory.model.loader - trainable params: 4194304 || all params: 7067621376 || trainable%: 0.0593\n",
      "[INFO|trainer.py:641] 2024-06-01 14:48:52,271 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2078] 2024-06-01 14:48:52,391 >> ***** Running training *****\n",
      "[INFO|trainer.py:2079] 2024-06-01 14:48:52,391 >>   Num examples = 5\n",
      "[INFO|trainer.py:2080] 2024-06-01 14:48:52,391 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2081] 2024-06-01 14:48:52,391 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:2084] 2024-06-01 14:48:52,391 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2085] 2024-06-01 14:48:52,391 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2086] 2024-06-01 14:48:52,391 >>   Total optimization steps = 3\n",
      "[INFO|trainer.py:2087] 2024-06-01 14:48:52,393 >>   Number of trainable parameters = 4,194,304\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]/root/anaconda3/envs/huggingface/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "[WARNING|logging.py:329] 2024-06-01 14:48:53,723 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:12<00:00,  3.61s/it][INFO|trainer.py:2329] 2024-06-01 14:49:04,894 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 12.5009, 'train_samples_per_second': 1.2, 'train_steps_per_second': 0.24, 'train_loss': 0.7740340232849121, 'epoch': 2.0}\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:12<00:00,  4.17s/it]\n",
      "[INFO|trainer.py:3410] 2024-06-01 14:49:04,895 >> Saving model checkpoint to saves/llava1_5-7b/lora/sft\n",
      "/root/anaconda3/envs/huggingface/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:140: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-01 14:49:05,801 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-01 14:49:05,805 >> Model config LlavaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlavaForConditionalGeneration\"\n",
      "  ],\n",
      "  \"ignore_index\": -100,\n",
      "  \"image_token_index\": 32000,\n",
      "  \"model_type\": \"llava\",\n",
      "  \"pad_token_id\": 32001,\n",
      "  \"projector_hidden_act\": \"gelu\",\n",
      "  \"text_config\": {\n",
      "    \"_name_or_path\": \"lmsys/vicuna-7b-v1.5\",\n",
      "    \"architectures\": [\n",
      "      \"LlamaForCausalLM\"\n",
      "    ],\n",
      "    \"max_position_embeddings\": 4096,\n",
      "    \"model_type\": \"llama\",\n",
      "    \"rms_norm_eps\": 1e-05,\n",
      "    \"torch_dtype\": \"float16\",\n",
      "    \"vocab_size\": 32064\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.41.1\",\n",
      "  \"vision_config\": {\n",
      "    \"hidden_size\": 1024,\n",
      "    \"image_size\": 336,\n",
      "    \"intermediate_size\": 4096,\n",
      "    \"model_type\": \"clip_vision_model\",\n",
      "    \"num_attention_heads\": 16,\n",
      "    \"num_hidden_layers\": 24,\n",
      "    \"patch_size\": 14,\n",
      "    \"projection_dim\": 768,\n",
      "    \"vocab_size\": 32000\n",
      "  },\n",
      "  \"vision_feature_layer\": -2,\n",
      "  \"vision_feature_select_strategy\": \"default\"\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-06-01 14:49:05,830 >> tokenizer config file saved in saves/llava1_5-7b/lora/sft/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-06-01 14:49:05,830 >> Special tokens file saved in saves/llava1_5-7b/lora/sft/special_tokens_map.json\n",
      "[INFO|image_processing_utils.py:257] 2024-06-01 14:49:05,865 >> Image processor saved in saves/llava1_5-7b/lora/sft/preprocessor_config.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  total_flos               =    47747GF\n",
      "  train_loss               =      0.774\n",
      "  train_runtime            = 0:00:12.50\n",
      "  train_samples_per_second =        1.2\n",
      "  train_steps_per_second   =       0.24\n",
      "06/01/2024 14:49:05 - WARNING - llamafactory.extras.ploting - No metric loss to plot.\n",
      "06/01/2024 14:49:05 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.\n",
      "[INFO|trainer.py:3719] 2024-06-01 14:49:05,870 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3721] 2024-06-01 14:49:05,870 >>   Num examples = 1\n",
      "[INFO|trainer.py:3724] 2024-06-01 14:49:05,870 >>   Batch size = 1\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 3572.66it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        2.0\n",
      "  eval_loss               =     2.6885\n",
      "  eval_runtime            = 0:00:00.43\n",
      "  eval_samples_per_second =      2.315\n",
      "  eval_steps_per_second   =      2.315\n",
      "[INFO|modelcard.py:450] 2024-06-01 14:49:06,303 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 llamafactory-cli train examples/lora_single_gpu/llava1_5_lora_sft.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "691fed02-69aa-4b28-a0c4-758408624012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/huggingface/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:100: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34820213d9e949fbb2f390df11b6e246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并后的模型已保存到 saves/llava1_5-7b/lora/merge\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlavaForConditionalGeneration, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# 加载基础模型\n",
    "base_model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "base_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ").to(0)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# 加载LoRA微调的配置\n",
    "lora_model_dir = \"saves/llava1_5-7b/lora/sft\"\n",
    "peft_config = PeftConfig.from_pretrained(lora_model_dir)\n",
    "\n",
    "# 加载LoRA微调模型\n",
    "lora_model = PeftModel.from_pretrained(base_model, lora_model_dir)\n",
    "\n",
    "# 合并基础模型和LoRA权重\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "\n",
    "# 保存合并后的模型\n",
    "merged_model_dir = \"saves/llava1_5-7b/lora/merge\"\n",
    "merged_model.save_pretrained(merged_model_dir)\n",
    "tokenizer.save_pretrained(merged_model_dir)\n",
    "\n",
    "print(f\"合并后的模型已保存到 {merged_model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1951ab2-a3c2-40d1-b692-e0938e7ce470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
