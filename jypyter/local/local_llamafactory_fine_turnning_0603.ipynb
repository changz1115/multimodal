{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1cc0627-bacd-4e98-92f8-78ed239e26c5",
   "metadata": {},
   "source": [
    "# 本地微调 llava i5 9600k + RTX4060ti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df4b6fe-be8d-41c0-b94a-9b93177ae1e1",
   "metadata": {},
   "source": [
    "## 确认conda环境以及当前session变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbf09908-baaa-4662-ab91-f2793d9da1d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     /root/anaconda3\n",
      "huggingface           *  /root/anaconda3/envs/huggingface\n",
      "\n",
      "declare -x CLICOLOR=\"1\"\n",
      "declare -x CLICOLOR_FORCE=\"1\"\n",
      "declare -x CONDA_DEFAULT_ENV=\"huggingface\"\n",
      "declare -x CONDA_EXE=\"/root/anaconda3/bin/conda\"\n",
      "declare -x CONDA_PREFIX=\"/root/anaconda3/envs/huggingface\"\n",
      "declare -x CONDA_PREFIX_1=\"/root/anaconda3\"\n",
      "declare -x CONDA_PROMPT_MODIFIER=\"(huggingface) \"\n",
      "declare -x CONDA_PYTHON_EXE=\"/root/anaconda3/bin/python\"\n",
      "declare -x CONDA_SHLVL=\"2\"\n",
      "declare -x DBUS_SESSION_BUS_ADDRESS=\"unix:path=/run/user/0/bus\"\n",
      "declare -x DISPLAY=\"localhost:10.0\"\n",
      "declare -x FORCE_COLOR=\"1\"\n",
      "declare -x GIT_PAGER=\"cat\"\n",
      "declare -x GSETTINGS_SCHEMA_DIR=\"/root/anaconda3/envs/huggingface/share/glib-2.0/schemas\"\n",
      "declare -x GSETTINGS_SCHEMA_DIR_CONDA_BACKUP=\"\"\n",
      "declare -x HOME=\"/root\"\n",
      "declare -x JPY_PARENT_PID=\"2743\"\n",
      "declare -x JPY_SESSION_NAME=\"/root/local_llamafactory_fine_turnning.ipynb\"\n",
      "declare -x LANG=\"en_US.UTF-8\"\n",
      "declare -x LESSCLOSE=\"/usr/bin/lesspipe %s %s\"\n",
      "declare -x LESSOPEN=\"| /usr/bin/lesspipe %s\"\n",
      "declare -x LOGNAME=\"root\"\n",
      "declare -x LS_COLORS=\"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.webp=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:\"\n",
      "declare -x MOTD_SHOWN=\"pam\"\n",
      "declare -x MPLBACKEND=\"module://matplotlib_inline.backend_inline\"\n",
      "declare -x OLDPWD\n",
      "declare -x PAGER=\"cat\"\n",
      "declare -x PATH=\"/root/anaconda3/envs/huggingface/bin:/root/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\"\n",
      "declare -x PWD=\"/root\"\n",
      "declare -x PYDEVD_USE_FRAME_EVAL=\"NO\"\n",
      "declare -x SHELL=\"/bin/bash\"\n",
      "declare -x SHLVL=\"3\"\n",
      "declare -x SSH_CLIENT=\"10.0.0.18 62424 22\"\n",
      "declare -x SSH_CONNECTION=\"10.0.0.18 62424 10.0.0.17 22\"\n",
      "declare -x SSH_TTY=\"/dev/pts/0\"\n",
      "declare -x TERM=\"xterm-color\"\n",
      "declare -x TERM_PROGRAM=\"tmux\"\n",
      "declare -x TERM_PROGRAM_VERSION=\"3.2a\"\n",
      "declare -x TMUX=\"/tmp/tmux-0/default,2711,0\"\n",
      "declare -x TMUX_PANE=\"%0\"\n",
      "declare -x USER=\"root\"\n",
      "declare -x XDG_DATA_DIRS=\"/usr/local/share:/usr/share:/var/lib/snapd/desktop\"\n",
      "declare -x XDG_RUNTIME_DIR=\"/run/user/0\"\n",
      "declare -x XDG_SESSION_CLASS=\"user\"\n",
      "declare -x XDG_SESSION_ID=\"1\"\n",
      "declare -x XDG_SESSION_TYPE=\"tty\"\n",
      "declare -x _=\"/root/anaconda3/envs/huggingface/bin/jupyter-lab\"\n",
      "declare -x _CE_CONDA=\"\"\n",
      "declare -x _CE_M=\"\"\n",
      "declare -x http_proxy=\"http://10.0.0.16:8080/\"\n",
      "declare -x https_proxy=\"http://10.0.0.16:8080/\"\n",
      "declare -x no_proxy=\"localhost,127.0.0.0/8,10.10.0.0/16,10.18.0.0/16,cluster.local,.svc\"\n"
     ]
    }
   ],
   "source": [
    "!conda info --envs\n",
    "!export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a05ad3-e1c7-4cc8-80a7-55428c78dd1b",
   "metadata": {},
   "source": [
    "## 非colab环境 登陆huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25081477-2006-4539-9294-207adf43f999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in as: changzheng\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, HfFolder\n",
    "\n",
    "# 替换为你的 Hugging Face 用户令牌\n",
    "token = \"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\"\n",
    "\n",
    "# 保存令牌\n",
    "HfFolder.save_token(token)\n",
    "\n",
    "# 创建 HfApi 实例\n",
    "api = HfApi()\n",
    "\n",
    "# 验证登录是否成功\n",
    "user_info = api.whoami()\n",
    "print(\"Successfully logged in as:\", user_info[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e68895-cb9f-4e0d-a5e9-74e3cb71b224",
   "metadata": {},
   "source": [
    "## 部署 llama-factory 环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c896b0d1-2c56-4427-bf2e-dd02623e4a1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
    "# %cd LLaMA-Factory\n",
    "# %pip install -e .[torch,metrics,bitsandbytes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7dad8ff-f2fe-4f1d-96ca-8e1b1c7983ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 环境满足\n",
      "Mon Jun  3 11:11:48 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 Ti     Off |   00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   40C    P8              8W /  165W |       5MiB /  16380MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "try:\n",
    "  assert torch.cuda.is_available() is True\n",
    "  print(\"GPU 环境满足\")\n",
    "  !nvidia-smi\n",
    "except AssertionError:\n",
    "  print(\"需要 GPU 环境\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965a74de-d173-4327-9200-438e2fedfb38",
   "metadata": {},
   "source": [
    "## 开始微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95f33f68-3cf3-4832-b75e-cab8f7598220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/LLaMA-Factory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/huggingface/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/huggingface/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "06/03/2024 11:18:33 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-03 11:18:34,919 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-03 11:18:34,919 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-03 11:18:34,919 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-03 11:18:34,919 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-03 11:18:34,919 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/tokenizer_config.json\n",
      "[WARNING|logging.py:314] 2024-06-03 11:18:34,960 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|image_processing_utils.py:374] 2024-06-03 11:18:36,144 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/preprocessor_config.json\n",
      "[INFO|image_processing_utils.py:374] 2024-06-03 11:18:36,553 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/preprocessor_config.json\n",
      "[INFO|image_processing_utils.py:424] 2024-06-03 11:18:36,556 >> Image processor CLIPImageProcessor {\n",
      "  \"_valid_processor_keys\": [\n",
      "    \"images\",\n",
      "    \"do_resize\",\n",
      "    \"size\",\n",
      "    \"resample\",\n",
      "    \"do_center_crop\",\n",
      "    \"crop_size\",\n",
      "    \"do_rescale\",\n",
      "    \"rescale_factor\",\n",
      "    \"do_normalize\",\n",
      "    \"image_mean\",\n",
      "    \"image_std\",\n",
      "    \"do_convert_rgb\",\n",
      "    \"return_tensors\",\n",
      "    \"data_format\",\n",
      "    \"input_data_format\"\n",
      "  ],\n",
      "  \"crop_size\": {\n",
      "    \"height\": 336,\n",
      "    \"width\": 336\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"processor_class\": \"LlavaProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 336\n",
      "  }\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-03 11:18:36,965 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-03 11:18:36,965 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-03 11:18:36,965 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-03 11:18:36,965 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-03 11:18:36,965 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/tokenizer_config.json\n",
      "[WARNING|logging.py:314] 2024-06-03 11:18:37,002 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|processing_utils.py:400] 2024-06-03 11:18:37,421 >> Processor LlavaProcessor:\n",
      "- image_processor: CLIPImageProcessor {\n",
      "  \"_valid_processor_keys\": [\n",
      "    \"images\",\n",
      "    \"do_resize\",\n",
      "    \"size\",\n",
      "    \"resample\",\n",
      "    \"do_center_crop\",\n",
      "    \"crop_size\",\n",
      "    \"do_rescale\",\n",
      "    \"rescale_factor\",\n",
      "    \"do_normalize\",\n",
      "    \"image_mean\",\n",
      "    \"image_std\",\n",
      "    \"do_convert_rgb\",\n",
      "    \"return_tensors\",\n",
      "    \"data_format\",\n",
      "    \"input_data_format\"\n",
      "  ],\n",
      "  \"crop_size\": {\n",
      "    \"height\": 336,\n",
      "    \"width\": 336\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"processor_class\": \"LlavaProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 336\n",
      "  }\n",
      "}\n",
      "\n",
      "- tokenizer: LlamaTokenizerFast(name_or_path='llava-hf/llava-1.5-7b-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32000: AddedToken(\"<image>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32001: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "\n",
      "{\n",
      "  \"processor_class\": \"LlavaProcessor\"\n",
      "}\n",
      "\n",
      "06/03/2024 11:18:37 - INFO - llamafactory.data.loader - Loading dataset mllm_demo.json...\n",
      "Generating train split: 6 examples [00:00, 156.97 examples/s]\n",
      "num_proc must be <= 6. Reducing num_proc to 6 for dataset of size 6.\n",
      "Converting format of dataset (num_proc=6): 100%|█| 6/6 [00:00<00:00, 49.05 examp\n",
      "num_proc must be <= 6. Reducing num_proc to 6 for dataset of size 6.\n",
      "Running tokenizer on dataset (num_proc=6): 100%|█| 6/6 [00:00<00:00, 14.99 examp\n",
      "input_ids:\n",
      "[319, 13563, 1546, 263, 12758, 1404, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 1404, 29915, 29879, 5155, 29889, 3148, 1001, 29901, 29871, 32000, 11644, 526, 896, 29973, 319, 1799, 9047, 13566, 29901, 2688, 29915, 276, 476, 1662, 322, 402, 2267, 29920, 1335, 515, 19584, 13564, 436, 636, 2, 3148, 1001, 29901, 1724, 526, 896, 2599, 29973, 319, 1799, 9047, 13566, 29901, 2688, 526, 10894, 1218, 373, 278, 269, 11953, 1746, 29889, 2]\n",
      "inputs:\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image> Who are they? ASSISTANT: They're Kane and Gretzka from Bayern Munich..</s> USER: What are they doing? ASSISTANT: They are celebrating on the soccer field.</s>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2688, 29915, 276, 476, 1662, 322, 402, 2267, 29920, 1335, 515, 19584, 13564, 436, 636, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2688, 526, 10894, 1218, 373, 278, 269, 11953, 1746, 29889, 2]\n",
      "labels:\n",
      "They're Kane and Gretzka from Bayern Munich..</s> They are celebrating on the soccer field.</s>\n",
      "/root/anaconda3/envs/huggingface/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-03 11:18:41,061 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/config.json\n",
      "/root/anaconda3/envs/huggingface/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:100: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:796] 2024-06-03 11:18:41,085 >> Model config LlavaConfig {\n",
      "  \"_name_or_path\": \"llava-hf/llava-1.5-7b-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlavaForConditionalGeneration\"\n",
      "  ],\n",
      "  \"ignore_index\": -100,\n",
      "  \"image_token_index\": 32000,\n",
      "  \"model_type\": \"llava\",\n",
      "  \"pad_token_id\": 32001,\n",
      "  \"projector_hidden_act\": \"gelu\",\n",
      "  \"text_config\": {\n",
      "    \"_name_or_path\": \"lmsys/vicuna-7b-v1.5\",\n",
      "    \"architectures\": [\n",
      "      \"LlamaForCausalLM\"\n",
      "    ],\n",
      "    \"max_position_embeddings\": 4096,\n",
      "    \"model_type\": \"llama\",\n",
      "    \"rms_norm_eps\": 1e-05,\n",
      "    \"torch_dtype\": \"float16\",\n",
      "    \"vocab_size\": 32064\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.41.1\",\n",
      "  \"vision_config\": {\n",
      "    \"hidden_size\": 1024,\n",
      "    \"image_size\": 336,\n",
      "    \"intermediate_size\": 4096,\n",
      "    \"model_type\": \"clip_vision_model\",\n",
      "    \"num_attention_heads\": 16,\n",
      "    \"num_hidden_layers\": 24,\n",
      "    \"patch_size\": 14,\n",
      "    \"projection_dim\": 768,\n",
      "    \"vocab_size\": 32000\n",
      "  },\n",
      "  \"vision_feature_layer\": -2,\n",
      "  \"vision_feature_select_strategy\": \"default\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3474] 2024-06-03 11:18:41,108 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1519] 2024-06-03 11:18:41,111 >> Instantiating LlavaForConditionalGeneration model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:962] 2024-06-03 11:18:41,113 >> Generate config GenerationConfig {\n",
      "  \"pad_token_id\": 32001\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:962] 2024-06-03 11:18:41,323 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:33<00:00, 11.13s/it]\n",
      "[INFO|modeling_utils.py:4280] 2024-06-03 11:19:14,837 >> All model checkpoint weights were used when initializing LlavaForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:4288] 2024-06-03 11:19:14,837 >> All the weights of LlavaForConditionalGeneration were initialized from the model checkpoint at llava-hf/llava-1.5-7b-hf.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlavaForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:917] 2024-06-03 11:19:15,466 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-06-03 11:19:15,467 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 32001\n",
      "}\n",
      "\n",
      "06/03/2024 11:19:15 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\n",
      "06/03/2024 11:19:15 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\n",
      "06/03/2024 11:19:15 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "06/03/2024 11:19:15 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "06/03/2024 11:19:16 - INFO - llamafactory.model.loader - trainable params: 4194304 || all params: 7067621376 || trainable%: 0.0593\n",
      "[INFO|trainer.py:641] 2024-06-03 11:19:17,009 >> Using auto half precision backend\n",
      "06/03/2024 11:19:17 - WARNING - llamafactory.extras.callbacks - Previous trainer log in this folder will be deleted.\n",
      "[INFO|trainer.py:2078] 2024-06-03 11:19:17,131 >> ***** Running training *****\n",
      "[INFO|trainer.py:2079] 2024-06-03 11:19:17,131 >>   Num examples = 5\n",
      "[INFO|trainer.py:2080] 2024-06-03 11:19:17,131 >>   Num Epochs = 100\n",
      "[INFO|trainer.py:2081] 2024-06-03 11:19:17,131 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:2084] 2024-06-03 11:19:17,131 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2085] 2024-06-03 11:19:17,131 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2086] 2024-06-03 11:19:17,131 >>   Total optimization steps = 100\n",
      "[INFO|trainer.py:2087] 2024-06-03 11:19:17,133 >>   Number of trainable parameters = 4,194,304\n",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]/root/anaconda3/envs/huggingface/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "[WARNING|logging.py:329] 2024-06-03 11:19:18,388 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "{'loss': 0.7326, 'grad_norm': 0.30989882349967957, 'learning_rate': 9.759636527645633e-05, 'epoch': 6.4}\n",
      "{'loss': 0.5497, 'grad_norm': 0.15373829007148743, 'learning_rate': 9.05246588405146e-05, 'epoch': 13.0}\n",
      "{'loss': 0.3908, 'grad_norm': 0.3914969563484192, 'learning_rate': 7.947823644532198e-05, 'epoch': 20.0}\n",
      "{'loss': 0.2242, 'grad_norm': 0.3665667772293091, 'learning_rate': 6.554054685128856e-05, 'epoch': 27.0}\n",
      "{'loss': 0.114, 'grad_norm': 0.23102693259716034, 'learning_rate': 5.007861840237924e-05, 'epoch': 33.6}\n",
      "{'loss': 0.0544, 'grad_norm': 0.24535897374153137, 'learning_rate': 3.460897894916863e-05, 'epoch': 40.0}\n",
      "{'loss': 0.0303, 'grad_norm': 0.035726986825466156, 'learning_rate': 2.0648912648459074e-05, 'epoch': 46.4}\n",
      "{'loss': 0.0178, 'grad_norm': 0.026743711903691292, 'learning_rate': 9.56764258331226e-06, 'epoch': 53.0}\n",
      "{'loss': 0.011, 'grad_norm': 0.1302090883255005, 'learning_rate': 2.452035422983734e-06, 'epoch': 60.0}\n",
      "{'loss': 0.0095, 'grad_norm': 0.12564139068126678, 'learning_rate': 0.0, 'epoch': 67.0}\n",
      "100%|█████████████████████████████████████████| 100/100 [06:11<00:00,  4.13s/it][INFO|trainer.py:2329] 2024-06-03 11:25:28,531 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 371.3981, 'train_samples_per_second': 1.346, 'train_steps_per_second': 0.269, 'train_loss': 0.2134428994357586, 'epoch': 67.0}\n",
      "100%|█████████████████████████████████████████| 100/100 [06:11<00:00,  3.71s/it]\n",
      "[INFO|trainer.py:3410] 2024-06-03 11:25:28,532 >> Saving model checkpoint to saves/llava1_5-7b/lora/sft\n",
      "/root/anaconda3/envs/huggingface/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:140: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-03 11:25:31,073 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-03 11:25:31,075 >> Model config LlavaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlavaForConditionalGeneration\"\n",
      "  ],\n",
      "  \"ignore_index\": -100,\n",
      "  \"image_token_index\": 32000,\n",
      "  \"model_type\": \"llava\",\n",
      "  \"pad_token_id\": 32001,\n",
      "  \"projector_hidden_act\": \"gelu\",\n",
      "  \"text_config\": {\n",
      "    \"_name_or_path\": \"lmsys/vicuna-7b-v1.5\",\n",
      "    \"architectures\": [\n",
      "      \"LlamaForCausalLM\"\n",
      "    ],\n",
      "    \"max_position_embeddings\": 4096,\n",
      "    \"model_type\": \"llama\",\n",
      "    \"rms_norm_eps\": 1e-05,\n",
      "    \"torch_dtype\": \"float16\",\n",
      "    \"vocab_size\": 32064\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.41.1\",\n",
      "  \"vision_config\": {\n",
      "    \"hidden_size\": 1024,\n",
      "    \"image_size\": 336,\n",
      "    \"intermediate_size\": 4096,\n",
      "    \"model_type\": \"clip_vision_model\",\n",
      "    \"num_attention_heads\": 16,\n",
      "    \"num_hidden_layers\": 24,\n",
      "    \"patch_size\": 14,\n",
      "    \"projection_dim\": 768,\n",
      "    \"vocab_size\": 32000\n",
      "  },\n",
      "  \"vision_feature_layer\": -2,\n",
      "  \"vision_feature_select_strategy\": \"default\"\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-06-03 11:25:31,136 >> tokenizer config file saved in saves/llava1_5-7b/lora/sft/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-06-03 11:25:31,136 >> Special tokens file saved in saves/llava1_5-7b/lora/sft/special_tokens_map.json\n",
      "[INFO|image_processing_utils.py:257] 2024-06-03 11:25:31,156 >> Image processor saved in saves/llava1_5-7b/lora/sft/preprocessor_config.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       67.0\n",
      "  total_flos               =  1599546GF\n",
      "  train_loss               =     0.2134\n",
      "  train_runtime            = 0:06:11.39\n",
      "  train_samples_per_second =      1.346\n",
      "  train_steps_per_second   =      0.269\n",
      "Figure saved at: saves/llava1_5-7b/lora/sft/training_loss.png\n",
      "06/03/2024 11:25:31 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.\n",
      "[INFO|trainer.py:3719] 2024-06-03 11:25:31,268 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3721] 2024-06-03 11:25:31,268 >>   Num examples = 1\n",
      "[INFO|trainer.py:3724] 2024-06-03 11:25:31,268 >>   Batch size = 1\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 3597.17it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =       67.0\n",
      "  eval_loss               =     2.2439\n",
      "  eval_runtime            = 0:00:00.43\n",
      "  eval_samples_per_second =      2.306\n",
      "  eval_steps_per_second   =      2.306\n",
      "[INFO|modelcard.py:450] 2024-06-03 11:25:31,703 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "%cd /root/LLaMA-Factory/\n",
    "!CUDA_VISIBLE_DEVICES=0 llamafactory-cli train /root/LLaMA-Factory/examples/lora_single_gpu/llava1_5_lora_sft.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ac1507-2b57-41bb-bda1-6adade983585",
   "metadata": {},
   "source": [
    "## 合并模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "691fed02-69aa-4b28-a0c4-758408624012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/huggingface/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:100: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6d158835014f8a9359341ea26d6a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并后的模型已保存到 saves/llava1_5-7b/lora/merge\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlavaForConditionalGeneration, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# 加载基础模型\n",
    "base_model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "# base_model_name = \"microsoft/layoutlmv3-base\"\n",
    "base_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ").to(0)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# 加载LoRA微调的配置\n",
    "lora_model_dir = \"saves/llava1_5-7b/lora/sft\"\n",
    "peft_config = PeftConfig.from_pretrained(lora_model_dir)\n",
    "\n",
    "# 加载LoRA微调模型\n",
    "lora_model = PeftModel.from_pretrained(base_model, lora_model_dir)\n",
    "\n",
    "# 合并基础模型和LoRA权重\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "\n",
    "# 保存合并后的模型\n",
    "merged_model_dir = \"saves/llava1_5-7b/lora/merge\"\n",
    "merged_model.save_pretrained(merged_model_dir)\n",
    "tokenizer.save_pretrained(merged_model_dir)\n",
    "\n",
    "print(f\"合并后的模型已保存到 {merged_model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200391d6-9fb8-4400-b75a-bc4dd596b2a8",
   "metadata": {},
   "source": [
    "## 测试一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b812abd6-dc47-426a-9678-243f3ac861de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f074e1f03f584902974ea21a1c250503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER:  \n",
      "Please describe this image\n",
      "ASSISTANT: The image features a smiling Chinese astronaut, Zhai Zhigang, standing in front of a microphone. He is wearing a blue and white jacket with a red and white emblem on the sleeve, representing the Chinese Space Agency. Zhai is waving while giving a speech, likely discussing his time on the International Space Station.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "model_id = \"/root/LLaMA-Factory/saves/llava1_5-7b/lora/merge/\"\n",
    "# model_id = \"/content/LLaMA-Factory/saves/llava1_5-7b/lora/merge\"\n",
    "\n",
    "prompt = \"USER: <image>\\nPlease describe this image\\nASSISTANT:\"\n",
    "image_file = \"/root/LLaMA-Factory/data/mllm_demo_data/3.jpg\"\n",
    "# image_file = \"/content/LLaMA-Factory/data/mllm_demo_data/3.jpg\"\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_id,\n",
    "    cache_dir=model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ").to(0)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "raw_image = Image.open(image_file)\n",
    "inputs = processor(prompt, raw_image, return_tensors='pt').to(0, torch.float16)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
    "print(processor.decode(output[0][2:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c67d407-7704-4b97-9f57-f8023535cb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER:  \n",
      "图片中是常铮吗？\n",
      "ASSISTANT: 是的，常铮是中国宇航员。\n"
     ]
    }
   ],
   "source": [
    "prompt = \"USER: <image>\\n图片中是常铮吗？\\nASSISTANT:\"\n",
    "inputs = processor(prompt, raw_image, return_tensors='pt').to(0, torch.float16)\n",
    "output = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
    "print(processor.decode(output[0][2:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd2ed1c-ef8e-4263-8143-f0742b071722",
   "metadata": {},
   "source": [
    "## 导出 gguf zhangjun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
